<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home</title>
    <link>/</link>
    <description>Home</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Aug 2021 10:21:48 +0100</lastBuildDate>
    
    <atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Modsecurity, DetectionOnly and enforcing select rules</title>
      <link>/posts/modsecurity-detectiononly-and-enforcing-select-rules/</link>
      <pubDate>Tue, 17 Aug 2021 10:21:48 +0100</pubDate>
      
      <guid>/posts/modsecurity-detectiononly-and-enforcing-select-rules/</guid>
      <description>&lt;p&gt;I recently had a reason to want to achieve the following:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ModSecurity globally in &lt;code&gt;DetectionOnly&lt;/code&gt;  mode (not enforcing rules, just logging)
&lt;ul&gt;
&lt;li&gt;Continue to operate the CRS in &lt;code&gt;DetectionOnly&lt;/code&gt; mode.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;For a specific ruleset:
&lt;ul&gt;
&lt;li&gt;Enforcing a default deny on inbound requests to an API.&lt;/li&gt;
&lt;li&gt;Enforcing allow rules for specific API routes and methods of the API&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So I wanted all of our inbound CRS rules to continue to work in &lt;code&gt;DetectionOnly&lt;/code&gt; mode, while I had a custom set of rules that would deny all access, with a set of whitelists to specific methods/paths.&lt;/p&gt;
&lt;p&gt;It wasn&amp;rsquo;t entirely clear from the documentation or  searches this was even possible, but after some trial and error turns out it is.&lt;/p&gt;
&lt;p&gt;First, you can enable or disable the SecAuditEngine on a rule by rule basis by using &lt;code&gt;ctl:ruleEngine=On&lt;/code&gt;.  So while the global setting &lt;code&gt;SecRuleEngine DetectionOnly&lt;/code&gt; is configured, you can set &lt;code&gt;ctl:ruleEngine=On&lt;/code&gt; on individual rules to enable the engine and allow disruptive actions to take place.&lt;/p&gt;
&lt;p&gt;Second you need to understand both ModSecurity Phases&lt;sup&gt;2,3&lt;/sup&gt; and RuleIDs&lt;sup&gt;4&lt;/sup&gt;&lt;/p&gt;
&lt;p&gt;The short version is:&lt;/p&gt;
&lt;p&gt;Rules are executed in phase first, rule id second order.  All rules in Phase 1, regardless of Rule ID, will be executed before any rules in Phase 2.  Took me a bit to find this documented clearly.&lt;/p&gt;
&lt;p&gt;So to achieve the inbound deny all, I created the following rule.  It&amp;rsquo;s in Phase 2 and the Rule ID is higher than any other Phase 2 rule, so it should execute last.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SecRule REQUEST_URI &amp;quot;@beginsWith /&amp;quot; \ 
  &amp;quot;id:9999999,\ 
  msg:&#39;Default Block/Deny All&#39;,\ 
  phase:2,\ 
  drop,\ 
  nolog,noauditlog,\ 
  ctl:ruleEngine=On,\ 
  t:none,t:lowercase,t:normalizePath&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Next step is to create the whitelist rules, a couple of examples below.  Each of them is in Phase 2 and has a Rule ID higher than the CRS Rule Ids, and lower than the above default deny Rule ID.  They should execute after the CRS rules, and before the above default deny.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;SecRule REQUEST_METHOD &amp;quot;^(?:DELETE|OPTIONS)$&amp;quot; \
  &amp;quot;id:5000001,\
  msg:&amp;quot;ALLOW DELETE /path/one&amp;quot;,\
  phase:2,\
  allow,\
  t:none,\
  ctl:ruleEngine=On,\
  log,auditlog,\
  chain&amp;quot;
	SecRule REQUEST_URI &amp;quot;@streq /path/one&amp;quot; &amp;quot;t:none,t:lowercase,t:normalizePath&amp;quot;

SecRule REQUEST_METHOD &amp;quot;^(?:DELETE|OPTIONS)$&amp;quot; \
  &amp;quot;id:5000002,\
  msg:&amp;quot;ALLOW DELETE /path/two/{someId}/more/{anotherId}&amp;quot;,\
  phase:2,\
  allow,\
  t:none,\
  ctl:ruleEngine=On,\
  log,auditlog,\
  chain&amp;quot;
	SecRule REQUEST_URI &amp;quot;@rx ^/path/two/\d+/more/\d+$&amp;quot; &amp;quot;t:none,t:lowercase,t:normalizePath&amp;quot;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That&amp;rsquo;s it really.  Any inbound request to this API can trigger any of the CRS rules, which as they&amp;rsquo;re running in &lt;code&gt;DetectionOnly&lt;/code&gt; will only log and not actually block the request.  Next the request will traverse the custom set of whitelist rules, and if it matches it will be allowed, and rule matching will stop here.  Next, if the request does not match any of the whitelist rules it will end up triggering the default deny rule and be dropped.&lt;/p&gt;
&lt;h3 id=&#34;notes&#34;&gt;Notes:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/SpiderLabs/ModSecurity/wiki/Reference-Manual-(v2.x)&#34;&gt;https://github.com/SpiderLabs/ModSecurity/wiki/Reference-Manual-(v2.x)&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://malware.expert/modsecurity/processing-phases-modsecurity/&#34;&gt;https://malware.expert/modsecurity/processing-phases-modsecurity/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://subscription.packtpub.com/book/networking-and-servers/9781847194749/2/ch02lvl1sec10/phases-and-rule-ordering&#34;&gt;https://subscription.packtpub.com/book/networking-and-servers/9781847194749/2/ch02lvl1sec10/phases-and-rule-ordering&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://coreruleset.org/docs/ruleid.html&#34;&gt;https://coreruleset.org/docs/ruleid.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Alerting using SIEM Detections and ElastAlert</title>
      <link>/posts/alerting-using-siem-detections-and-elastalert/</link>
      <pubDate>Tue, 17 Aug 2021 08:32:41 +0100</pubDate>
      
      <guid>/posts/alerting-using-siem-detections-and-elastalert/</guid>
      <description>&lt;p&gt;ElasticSearch SIEM Detections and Alerts and Actions are quite useful features, except for the fact that actual alerting is behind a license paywall.  So while both of these features can run rules, check for conditions, and record the results in an index, neither of them actually provide &lt;em&gt;alerting&lt;/em&gt; support.&lt;/p&gt;
&lt;p&gt;Alerting requires a Gold License, which if alerting is the only thing you want, is an excessive cost.&lt;/p&gt;
&lt;p&gt;If you can&amp;rsquo;t move off ElasticSearch to &lt;a href=&#34;https://opensearch.org/&#34;&gt;OpenSearch&lt;/a&gt;, which has Alerting available for free, you can use tools such as &lt;a href=&#34;https://github.com/jertel/elastalert2&#34;&gt;ElastAlert2&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; to handle the Alerting requirements.&lt;/p&gt;
&lt;h1 id=&#34;siem-detections&#34;&gt;SIEM Detections&lt;/h1&gt;
&lt;p&gt;The following example is for SIEM Detections, and alerting with ElastAlert2.&lt;/p&gt;
&lt;p&gt;SIEM Detections record their results in an index called &lt;code&gt;.siem-signals-default&lt;/code&gt;.  The &lt;code&gt;-default&lt;/code&gt; part is based on the Kibana space, so if you&amp;rsquo;re using a Kibana space called &lt;code&gt;exampleA&lt;/code&gt;, the index name would be &lt;code&gt;.siem-signals-examplea&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Make sure to create a Kibana Index Pattern for this index, so you can explore it fully.&lt;/p&gt;
&lt;p&gt;The key field to be aware of is the &lt;code&gt;signal.rule.name&lt;/code&gt;, which is of course the SIEM Detection Rule name.  This is what we&amp;rsquo;ll use to create an ElastAlert rule.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll want to check what kind of ElastAlert rule you want to create, which you can find &lt;a href=&#34;https://elastalert2.readthedocs.io/en/latest/ruletypes.html#rule-types&#34;&gt;here ( https://elastalert2.readthedocs.io/en/latest/ruletypes.html#rule-types )&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For many of our SIEM Detection rules we use the ElastAlert &lt;code&gt;any&lt;/code&gt; rule type.  According to the ElastAlert documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The any rule will match everything. Every hit that the query returns will generate an alert.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In many cases this will not be what you want as it could generate a lot of noise, but in the case of SIEM Detections, if they&amp;rsquo;re tuned well, hopefully they won&amp;rsquo;t be generating hundreds of records that this ElastAlert rule would be alerting on.&lt;/p&gt;
&lt;p&gt;Below is an example ElastAlert rule that alerts us when there are Azure Subscription level IAM changes (as detected by a SIEM Detection rule).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: Azure Subscription IAM Change

index: .siem-signals-default-*

filter:
- query:
    query_string:
      query: &#39;signal.rule.name: &amp;quot;Azure Subscription IAM Change&amp;quot; AND event.outcome: *&#39;

type: any

realert:
  minutes: 0

alert:
- &amp;quot;slack&amp;quot;

alert_subject: &amp;quot;Azure Subscription IAM Change&amp;quot;

alert_text: &amp;quot;
{0}\n
Grantor:\n
User Name: {1}\n
Application Name: {2}\n
Application ID: {3}\n
\n
Grantee:\n
Principal Name: {4}\n
Principal ID: {5}\n
Principal Type: {6}\n
Role Name: {7}\n
Role ID: {8}\n
Subscription Name: {9}\n
Subscription ID: {10}\n
\n&amp;quot;

alert_missing_value: &amp;quot;N/A&amp;quot;

alert_text_args:
- &amp;quot;signal.rule.name&amp;quot;
- &amp;quot;user.name&amp;quot;
- &amp;quot;aad.application.name&amp;quot;
- &amp;quot;aad.application.id&amp;quot;
- &amp;quot;azure.iam.principal.name&amp;quot;
- &amp;quot;azure.iam.principal.id&amp;quot;
- &amp;quot;azure.iam.principal.type&amp;quot;
- &amp;quot;azure.iam.role.name&amp;quot;
- &amp;quot;azure.iam.role.id&amp;quot;
- &amp;quot;azure.subscription.name&amp;quot;
- &amp;quot;azure.subscription.id&amp;quot;

alert_text_type: alert_text_only
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;Note: ElastAlert uses the older Lucene query syntax, whereas modern Kibana uses Kibana Query Language (KQL) by default.  Make sure to switch to using Lucene in Kibana when exploring or writing searches for use with ElastAlert.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ElastAlert has an extensive set of possible alert targets, in the example I&amp;rsquo;m using Slack, but a few of the other common ones I use are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP POST&lt;/li&gt;
&lt;li&gt;Command&lt;/li&gt;
&lt;li&gt;Alerta (quite useful alert dashboard)&lt;/li&gt;
&lt;li&gt;Email&lt;/li&gt;
&lt;li&gt;Jira&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The full list can be found here: &lt;a href=&#34;https://elastalert2.readthedocs.io/en/latest/ruletypes.html#alerts&#34;&gt;ElastAlert Alerters&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;ElastAlert2 is the community fork of the original Yelp created ElastAlert which they abandoned a year or two ago, without any real effort to hand over to anyone to maintain.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Using Elasticsearch Upserts to Combine Multiple Event Lines Into One</title>
      <link>/posts/using-elasticsearch-upserts-to-combine-multiple-event-lines-into-one/</link>
      <pubDate>Tue, 24 Nov 2020 07:39:19 +0000</pubDate>
      
      <guid>/posts/using-elasticsearch-upserts-to-combine-multiple-event-lines-into-one/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Note
This approach is probably not appropriate for high volume / high throughput events.  It required in my case quite a lot of Logstash parsing, and Elasticsearch &lt;code&gt;doc_as_upsert&lt;/code&gt; use, both of which will have a significant performance penalty.  For low throughput use it works fine.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sometimes log sources split logically grouped events into separate lines, and sometimes those logically grouped event lines are mixed into the same log file with actual singular line events.&lt;/p&gt;
&lt;p&gt;This particular case is not dealt with well by Filebeat multiline support&lt;sup&gt;1&lt;/sup&gt;.  In fact it simply doesn&amp;rsquo;t work in this case.&lt;/p&gt;
&lt;p&gt;The structure I&amp;rsquo;m talking about is this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Line 1
Line 2
Line 3 - Some event starts
Line 4 - Content of event
Line 5 - End of event
Line 6
Line 7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Where Lines 1 and 2 are individual events, Lines 3, 4, and 5 are actually multiple lines of the same event, and Lines 6 and 7 are separate individual events again.&lt;/p&gt;
&lt;p&gt;Since Filebeat doesn&amp;rsquo;t deal with this type of setup, at all, I had to look elsewhere to see if I could combine Lines 3, 4, and 5 into one event.&lt;/p&gt;
&lt;p&gt;Logically the next place to look would be Logstash, as we have it in our ingestion pipeline and it has multiline capabilities.  However, we use a set of Azure Event Hubs (essentially Kafka for those not familiar) as our event queueing mechanism, with a group of Logstash processes consuming the events as they arrive.  There&amp;rsquo;s no grouping or ordering here, so Lines 3,4,5 may arrive:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out of order in time&lt;/li&gt;
&lt;li&gt;across multiuple different Logstash consumers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This means it impossible to combine these 3 lines into one event, as we may never see all 3 lines at a single logstash process/instance, and therefore can&amp;rsquo;t combine them.&lt;/p&gt;
&lt;p&gt;So they can&amp;rsquo;t be combined at source using Filebeat, and they can&amp;rsquo;t be combined during processing by using Logstashs multiline codec, which only leaves one place where all 3 lines will be in the same place: Elasticsearch itself.&lt;/p&gt;
&lt;p&gt;The approach I settled on was using (or perhaps abusing) Elasticseach&amp;rsquo;s &lt;code&gt;doc_as_upsert&lt;/code&gt;&lt;sup&gt;2&lt;/sup&gt; capability to incrementally add data to a single ES document.&lt;/p&gt;
&lt;p&gt;The key is to identify something that can group the multiple lines together, and use that information as the Document ID.&lt;/p&gt;
&lt;p&gt;In my case, we have the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Two common &amp;ldquo;phrases&amp;rdquo; in the event line, such that I can identify all lines reliably as being part of a logical group (i.e. they need to be processed as per the next step)&lt;/li&gt;
&lt;li&gt;A set of datetime and ip/port information thats common across the event lines, that can be used to create a shared &amp;ldquo;signature&amp;rdquo; (using Logstash fingerprint filter)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first step is to identify the common &amp;ldquo;phrases&amp;rdquo; that identify the event lines, and mark each event as part of an &amp;ldquo;upsert&amp;rdquo;.  I do this as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if ( [message] =~ /phrase1/ ) or ( [message] =~ /phrase2/ ) {
    mutate {
      add_tag =&amp;gt; [ &amp;quot;_upserts&amp;quot; ]
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The next step is to parse the common data of each event into a structure that allows the use of Logstash&amp;rsquo;s fingerprint filter.  I extract the datetime and ip/port information, and use Logstash&amp;rsquo;s &lt;code&gt;fingerprint&lt;/code&gt; filter to create an ECS style &lt;code&gt;[event][id]&lt;/code&gt; field.&lt;/p&gt;
&lt;p&gt;Fingerprint the event line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    fingerprint {
      method =&amp;gt; &amp;quot;SHA1&amp;quot;
      source =&amp;gt; [
        &amp;quot;[tmp_date_day]&amp;quot;,
        &amp;quot;[tmp_date_month]&amp;quot;,
        &amp;quot;[tmp_date_daynum]&amp;quot;,
        &amp;quot;[tmp_date_time]&amp;quot;,
        &amp;quot;[tmp_date_year]&amp;quot;,
        &amp;quot;[tmp_source_ip]&amp;quot;,
        &amp;quot;[tmp_source_port]&amp;quot;
      ]
      target =&amp;gt; &amp;quot;[event][id]&amp;quot;
    }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In my Elasticsearch outputs I then simply filter for that tag, and set a few paramters, as below.  This uses the &lt;code&gt;[event][id]&lt;/code&gt; as the document ID, and will perform an update if a document exists already with the same document id:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if &amp;quot;_upserts&amp;quot; in [tags] {
  elasticsearch {
    hosts =&amp;gt; [
      &amp;quot;es&amp;quot;
    ]
    index =&amp;gt; &amp;quot;&amp;lt;target index&amp;gt;&amp;quot;
    document_id =&amp;gt; &amp;quot;%{[event][id]}&amp;quot;
    doc_as_upsert =&amp;gt; true
    action =&amp;gt; &amp;quot;update&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That&amp;rsquo;s it.  Abusing Elasticsearch Update API to combine multiline log events into one document.  Please don&amp;rsquo;t do this if you have better options available!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/elastic/beats/pull/4019&#34;&gt;https://github.com/elastic/beats/pull/4019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#doc_as_upsert&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#doc_as_upsert&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Event Threat Enrichment using Logstash and Minemeld</title>
      <link>/posts/event-threat-enrichment-logstash-minemeld/</link>
      <pubDate>Fri, 25 Sep 2020 09:22:36 +0100</pubDate>
      
      <guid>/posts/event-threat-enrichment-logstash-minemeld/</guid>
      <description>&lt;p&gt;At my work we use the Elastic Stack for quite a few things, but one of the more recent-ish &amp;ldquo;official&amp;rdquo; roles is as our SIEM.  Elastic introduced SIEM specific funcationality to Kibana a few releases ago, around 7.4 if I rembember correctly.&lt;/p&gt;
&lt;p&gt;One of the features that the Elastic Stack doesn&amp;rsquo;t really support well (yet) is an enrichment system.  They did introduce an elasticsearch side &lt;a href=&#34;https://www.elastic.co/blog/introducing-the-enrich-processor-for-elasticsearch-ingest-nodes&#34;&gt;enrichment system&lt;/a&gt; in 7.5, but in my opinionn theres a few problems with it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It runs inside ElasticSeach using ES pipelines, which are harder to design and operate than something Logstash side, as wellas not quite being as flexible I&amp;rsquo;d like.&lt;/li&gt;
&lt;li&gt;As it runs in ES ingest nodes, it has a license cost impact&lt;/li&gt;
&lt;li&gt;The processors available have some limitations, such as no support for range queries currently, which is important in the use case I&amp;rsquo;ll be writing about here.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the requirements I have for our SIEM is to be able to identify events coming from &amp;ldquo;known risky IPs&amp;rdquo;.  Those risky IPs could be known botnets, spam sources, tor exit nodes, or anything else that you or threat intelligence providers/feeds classify as a risk.&lt;/p&gt;
&lt;p&gt;After much research I settled on the approach I&amp;rsquo;ll detail below.&lt;/p&gt;
&lt;p&gt;First I needed a way of collecting threat feeds in such a way that they&amp;rsquo;d be useable by the SIEM.  For this I settled on using &lt;a href=&#34;https://www.paloaltonetworks.com/products/secure-the-network/subscriptions/minemeld&#34;&gt;Minemeld&lt;/a&gt;, a product by Palo Alto networks, as they describe it &amp;ldquo;an open-source application that streamlines the aggregation, enforcement and sharing of threat intelligence&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;There are quite a few other options for this, but Minemeld seemed ideal for me because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s a pretty focused design, where some other options are far far more than just threat feed aggregation.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s pretty simple to use and setup&lt;/li&gt;
&lt;li&gt;It has support for delivering threat feed data to Logstash&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;minemeld&#34;&gt;Minemeld&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Note
This post only deals with IoCs of &amp;ldquo;IP type&amp;rdquo;.  Minemeld can consume, aggregate, and distribute IoCs of other tyes, such as URLs, domains, etc, but they are not dealt with by this article.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How to install, run and configure feeds/aggregators/etc in Minemeld is an exercise left to the reader.&lt;/p&gt;
&lt;p&gt;To enable Logstash output, you can use the built in prototype &lt;code&gt;stdlib.localLogStash&lt;/code&gt;, if your logstash instance is running on the same system as Minemeld and is reachable over localhost:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/234a165e8b86498c8956617b0d01e6ca.png&#34; alt=&#34;234a165e8b86498c8956617b0d01e6ca.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;However in my case, my logstash instances arent runnin on the same host, and in fact I have multiple target instances.&lt;/p&gt;
&lt;p&gt;In a case like this, you need to create a new prototype by pressing the &amp;ldquo;New&amp;rdquo; button highlighted in the above screenshot.&lt;/p&gt;
&lt;p&gt;Set your new &amp;ldquo;local prototype&amp;rdquo; name, and then the important part, set the &lt;code&gt;logstash_host:&lt;/code&gt; config field:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/465f6543348749ac96240d6db28031b0.png&#34; alt=&#34;465f6543348749ac96240d6db28031b0.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Hit save, and thats it.  You now have a method of outputting IoCs from various IP threat feeds to Logstash.  I&amp;rsquo;ve done this two times, with different &lt;code&gt;logstash_host:&lt;/code&gt; set for each new local prototype.  So I&amp;rsquo;m duplicating my IoCs to two different logstash and elasticsearch clusters.&lt;/p&gt;
&lt;h1 id=&#34;logstash---part-1&#34;&gt;Logstash - Part 1&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Important Note:
You will need to define an index template/mapping that ensures the start and end IP address fields created by the dissect filter below are actually of IP datatype.  IP Range queries used in Part 2 will not work without this.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;On the logstash side the configs for reading and storing the Minemeld provided IoCs are reasonably simple.&lt;/p&gt;
&lt;p&gt;Part 2 will look at how I do event enrichment.&lt;/p&gt;
&lt;p&gt;First, you need to listen on the TCP input port configured in your Minemeld local prototypes, in the case above:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input {
  tcp {
    port =&amp;gt; 5514
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You then need some filters to parse the message field, create a predictable document ID, and split the &lt;code&gt;@indicator&lt;/code&gt; into a start and end IP address, which will be used for ES range queries in Part 2.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filter {
  # Parse the message field as JSON into the target minemeld field
  json {
    source =&amp;gt; &amp;quot;message&amp;quot;
    target =&amp;gt; &amp;quot;minemeld&amp;quot;
  }
  
  # Generate a fingerprint on the @indicator field, this will be used as the Document ID in the Elasticsearch outputs.
  fingerprint {
    source =&amp;gt; &amp;quot;[minemeld][@indicator]&amp;quot;
    target =&amp;gt; &amp;quot;[@metadata][fingerprint]&amp;quot;
    method =&amp;gt; &amp;quot;MURMUR3&amp;quot;
  }

  # Split the IP indicator field on the -, so we have a start and end IP address
  dissect {
    mapping =&amp;gt; { &amp;quot;[minemeld][@indicator]&amp;quot; =&amp;gt; &amp;quot;%{[minemeld][indicator][ip][start]}-%{[minemeld][indicator][ip][end]}&amp;quot; }
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So now you just need to store that docuement in an ES index.  The slightly different part, as compared to for example storing log data in ES, is that IoCs can and will &amp;ldquo;expire&amp;rdquo;, and therefore need to be removed from the index when that happens.&lt;/p&gt;
&lt;p&gt;Minemeld caters for this by providing a &amp;ldquo;message&amp;rdquo;, and if the value of that message is &amp;ldquo;withdraw&amp;rdquo;, the IoC can be removed.  As can be seen in the Logstash output configs below, when we receive an event with &lt;code&gt;[minemeld][message] == &amp;quot;withdraw&amp;quot;&lt;/code&gt; we issue a delete against the ES index, as indicated by &lt;code&gt;action =&amp;gt; &amp;quot;delete&amp;quot;&lt;/code&gt;.  This uses the predictable document ID we create in the earlier filter, so we know which document to delete.&lt;/p&gt;
&lt;p&gt;Also note the index template specified in the configs, and refer to the note at the start of this section.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output {
    if [minemeld][message] == &amp;quot;withdraw&amp;quot; {
      elasticsearch {
        hosts =&amp;gt; [
          &amp;quot;http://&amp;lt;es&amp;gt;&amp;quot;
        ]
        index =&amp;gt; &amp;quot;minemeld&amp;quot;
        manage_template =&amp;gt; true
        template_name =&amp;gt; &amp;quot;minemeld&amp;quot;
        template =&amp;gt; &amp;quot;/etc/logstash/minemeld.indextemplate&amp;quot;
        document_id =&amp;gt; &amp;quot;%{[@metadata][fingerprint]}&amp;quot;
        action =&amp;gt; &amp;quot;delete&amp;quot;
      }
    } else {
      elasticsearch {
        hosts =&amp;gt; [
          &amp;quot;http://&amp;lt;es&amp;gt;&amp;quot;
        ]
        index =&amp;gt; &amp;quot;minemeld&amp;quot;
        manage_template =&amp;gt; true
        template_name =&amp;gt; &amp;quot;minemeld&amp;quot;
        template =&amp;gt; &amp;quot;/etc/logstash/minemeld.indextemplate&amp;quot;
        document_id =&amp;gt; &amp;quot;%{[@metadata][fingerprint]}&amp;quot;
      }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So at this point you should have some Minemeld provided IoCs stored in an Elasticsearch index, which can be used to enrich other events in real time.&lt;/p&gt;
&lt;h1 id=&#34;logstash---part-2&#34;&gt;Logstash - Part 2&lt;/h1&gt;
&lt;p&gt;So now onto how to enrich events.  In my case, certain events come with a &lt;code&gt;source.ip&lt;/code&gt; address, and I then do a ES lookup using the Logstash elasticsearch &lt;em&gt;filter&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filter {
      elasticsearch {
          hosts =&amp;gt; [
            &amp;quot;http://&amp;lt;es&amp;gt;/&amp;quot;
          ]
        index =&amp;gt; &amp;quot;minemeld&amp;quot;
        enable_sort =&amp;gt; &amp;quot;false&amp;quot;
        tag_on_failure =&amp;gt; [ &amp;quot;_threat_lookup_failure&amp;quot; ]
        add_tag =&amp;gt; [&amp;quot;_threat_found&amp;quot;]
        query_template =&amp;gt; &amp;quot;/etc/logstash/conf.d/threat_query.json&amp;quot;
        fields =&amp;gt; {
          &amp;quot;[minemeld][sources]&amp;quot; =&amp;gt; &amp;quot;[custom][threat][sources]&amp;quot;
        }
      }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The query, specified in &lt;code&gt;query_template =&amp;gt; &amp;quot;/etc/logstash/conf.d/threat_query.json&amp;quot;&lt;/code&gt; is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;query&amp;quot;: {
    &amp;quot;bool&amp;quot;: {
      &amp;quot;filter&amp;quot;: [
        { &amp;quot;range&amp;quot;: { &amp;quot;minemeld.indicator.ip.start&amp;quot;: { &amp;quot;lte&amp;quot;: &amp;quot;%{[source][ip]}&amp;quot; }}},
        { &amp;quot;range&amp;quot;: { &amp;quot;minemeld.indicator.ip.end&amp;quot;: { &amp;quot;gte&amp;quot;: &amp;quot;%{[source][ip]}&amp;quot; }}}
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;Note
This is where you need to ensure you&amp;rsquo;ve created these fields as IP datatypes, as mentioned previously.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All this does is excute the above query against the Minemeld ES index, and looks for a match where the Minemeld &lt;code&gt;minemeld.indicator.ip.start&lt;/code&gt; is less than or equal to, and the &lt;code&gt;minemeld.indicator.ip.end&lt;/code&gt; is greater than or equal to the source IP in the event that we&amp;rsquo;re attempting to enrich.&lt;/p&gt;
&lt;p&gt;Basically: &amp;ldquo;if the source IP is between the start and end range provided by Minemeld&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The actual enrichment is performed by the &lt;code&gt;fields&lt;/code&gt; parameter of the &lt;code&gt;elasticsearch&lt;/code&gt; filter.  In the example above, it sets a field &lt;code&gt;custom.threat.sources&lt;/code&gt; to the value of &lt;code&gt;minemeld.sources&lt;/code&gt; from the document in the Minemeld index, which is a list of source &amp;ldquo;names&amp;rdquo; provided by minemeld.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;That&amp;rsquo;s it.  We now have an indication if a source IP in an event is considered a threat by various threat feeds, which is obviously very useful for informing any decisions around responses.&lt;/p&gt;
&lt;p&gt;Some closing thoughts.&lt;/p&gt;
&lt;p&gt;Performance.  This approach is almost definitely not scalable to extremely high throughput levels, due to the overhead of network connections between logstash and elasticsearch, and the impact of querying elasticsearch for every event.  It would be signficantly better tyo take advantage of Logstashs&#39; translate filter or its memcache filter.&lt;/p&gt;
&lt;p&gt;However, both the dictionary and memcache filter suffer from the same limitation: theres no (easy?) way of doing range type queries.&lt;/p&gt;
&lt;p&gt;As I mentioned earlier, Elasticsearchs new enrichment pipeline features also doesnt support range queries, at this time.&lt;/p&gt;
&lt;p&gt;Since the Minemeld threat feeds provide IP ranges, the only options are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use a queryable source that supports range queries on IP addresses (or their integer representation thereof)&lt;/li&gt;
&lt;li&gt;Expand the Minemeld provided IP ranges into lists of singlular IPs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I actually experimented with option two, expanding the Minemeld ranges into complete lists of single /32 IPs. Depending on the number of the threat feeds being consumed and their size, you will end up with millions and millions of IP addresses in your new list, which may not be usable in logstash dictionaries or memcache.  YMMV.&lt;/p&gt;
&lt;p&gt;At the time I decided to continue using the Elasticsearch IP range query approach for now, as the number of events I&amp;rsquo;m enriching is low enough to not impact our performance or availability.  However in the future I&amp;rsquo;m going to want to apply this kind of enrichment against a much larger amount of events, so I&amp;rsquo;m going to have to revisit the approach I think.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Querying Cylance Protect Api From Shell</title>
      <link>/posts/querying-cylance-protect-api-from-shell/</link>
      <pubDate>Fri, 11 Sep 2020 17:32:50 +0100</pubDate>
      
      <guid>/posts/querying-cylance-protect-api-from-shell/</guid>
      <description>&lt;p&gt;We use Cylance as our AV type protection.  They&amp;rsquo;re one of the better solutions I&amp;rsquo;ve seen, but theres some strange gaps in my opinion.  There doesn&amp;rsquo;t seem to be a built in method for alerting.  One of the things we&amp;rsquo;d like to be able to alert on is when a devices goes &amp;ldquo;offline&amp;rdquo;, and apparently this information is not provided through Cylance&amp;rsquo;s syslog output.  It is however available from their API.&lt;/p&gt;
&lt;p&gt;Since we use Elasticsearch at Bede, and use it for the basis of a lot of alerting, I wanted to get the device status records from Cylance, into Elasticsearch so I could alert our security team when Cylance agents stopped reporting in.&lt;/p&gt;
&lt;p&gt;After a bit of reading, the Cylance API seemed simple enough so I whipped up a bit of shell using curl, jq, openssl, etc to authenticate/authorize, and be able to hit any of the API endpoints.&lt;/p&gt;
&lt;p&gt;Hopefully someone else find it useful as well, the code can be found here: &lt;a href=&#34;https://github.com/robrankin/bash-cylance-protect-api&#34;&gt;https://github.com/robrankin/bash-cylance-protect-api&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Credit for some very helpful notes to: &lt;a href=&#34;https://gist.github.com/indrayam/dd47bf6eef849a57c07016c0036f5207&#34;&gt;https://gist.github.com/indrayam/dd47bf6eef849a57c07016c0036f5207&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kibaba Authentication using OAuth2 Proxy in Kubernetes</title>
      <link>/posts/kibaba-oauth-kubernetes/</link>
      <pubDate>Thu, 06 Aug 2020 12:34:50 +0100</pubDate>
      
      <guid>/posts/kibaba-oauth-kubernetes/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;NOTE: There appears to be a bug with Kibanas impersonation features, and SIEM detection rules (and possibly elswhere): &lt;a href=&#34;https://github.com/elastic/kibana/issues/74828&#34;&gt;https://github.com/elastic/kibana/issues/74828&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recently I had reason to want to integrate Kibana with Azure Active Directory for authentication.  This might be easily possible if you have a commercial license with Elastic, but this wasn&amp;rsquo;t the case this time.&lt;/p&gt;
&lt;p&gt;After a little bit of research I found this article, from February 2017:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/blog/user-impersonation-with-x-pack-integrating-third-party-auth-with-kibana&#34;&gt;User Impersonation with X-Pack: Integrating Third Party Auth with Kibana&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Obviously it&amp;rsquo;s starting to get a little long in the tooth, but as long as user impersonation is still supported, the basic outline should work.&lt;/p&gt;
&lt;p&gt;The main trouble with the article is the specific setup used, illustrated by the image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/oauth_kibana_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;oauth2_proxy&lt;/code&gt; terminating the browser connection (and possibly TLS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;oauth2_proxy&lt;/code&gt; running in reverse proxy mode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is more what I was looking for:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/Kibana_Oauth2_Kubernetes.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this deployment, Kibana and OAuth2 Proxy would be deployed on Kubernetes, and would be made available behind the standard k8s ingress controller, &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;Ingress Nginx&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Therefore the browser connection would be terminated on the Ingress Nginx controller.  As a side note, this takes advantage of &lt;code&gt;cert-manager&lt;/code&gt; to automatically provision and manage TLS certificates, very handy.&lt;/p&gt;
&lt;p&gt;I also didn&amp;rsquo;t really want to run &lt;code&gt;oauth2_proxy&lt;/code&gt; in full reverse proxy mode, as that&amp;rsquo;s the job of the ingress controller.  Don&amp;rsquo;t need more proxies involved here.&lt;/p&gt;
&lt;p&gt;Luckily, nginx has just the feature for this, the &lt;a href=&#34;http://nginx.org/en/docs/http/ngx_http_auth_request_module.html&#34;&gt;ngx_http_auth_request&lt;/a&gt; module.  As per the documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ngx_http_auth_request_module module (1.5.4+) implements client authorization based on the result of a subrequest. If the subrequest returns a 2xx response code, the access is allowed. If it returns 401 or 403, the access is denied with the corresponding error code. Any other response code returned by the subrequest is considered an error.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, in theory as long as a header called &lt;code&gt;es-security-runas-user&lt;/code&gt; can be passed to Kibana, with a value of a valid username, and be authrorized to do so by using HTTP Basic auth as a user with rights to impersonate another use, we&amp;rsquo;re good to go.&lt;/p&gt;
&lt;p&gt;The short form is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &amp;ldquo;Service Account&amp;rdquo; is required in Kibana, with a Role that allows it to impersonate users.&lt;/li&gt;
&lt;li&gt;User accounts in Kibana for your users, with whatever roles they require.&lt;/li&gt;
&lt;li&gt;All requests from the ingress controller to Kibana must have 2 headers:
&lt;ul&gt;
&lt;li&gt;Basic Auth header for the &amp;ldquo;Service Account&amp;rdquo;&lt;/li&gt;
&lt;li&gt;An &lt;code&gt;es-security-runas-user&lt;/code&gt; with a valid username created in Kibana.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So let&amp;rsquo;s get down to it, then, here&amp;rsquo;s the specific configurations required to make this work!&lt;/p&gt;
&lt;p&gt;These will be a mix between Helm chart values, and some &amp;ldquo;raw-ish&amp;rdquo; Kube configs (deployed using the &lt;code&gt;raw&lt;/code&gt; Helm chart).&lt;/p&gt;
&lt;p&gt;OAuth2 Proxy Helm values.yaml&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;image:
  repository: &amp;quot;quay.io/pusher/oauth2_proxy&amp;quot;
  tag: &amp;quot;v6.0.0&amp;quot;
  pullPolicy: &amp;quot;IfNotPresent&amp;quot;

extraArgs:
  provider: &#39;azure&#39;
  email-domain: &#39;&amp;lt;your email domain here&amp;gt;&#39;
  azure-tenant: &#39;&amp;lt;your tenant id here&amp;gt;&#39;
  client-id: &#39;&amp;lt;your client id here&amp;gt;&#39;
  client-secret: &#39;&amp;lt;your client secret here&amp;gt;&#39;

  redirect-url: https://kibana.example.com/oauth2/callback

  cookie-secret: &#39;&amp;lt;cookie secret&amp;gt;&#39;
  cookie-domain: &amp;lt;cookie domain&amp;gt;
  cookie-samesite: none

  set-xauthrequest: true

  session-store-type: redis
  redis-connection-url: &#39;redis://redis-master:6379/0&#39;

  request-logging: true
  auth-logging: true
  standard-logging: true
  silence-ping-logging: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Ensure your callback URL does NOT have a trailing slash &lt;code&gt;/&lt;/code&gt;.  This caused me some problems.&lt;/li&gt;
&lt;li&gt;To create an app in Azure for OAuth2 Proxy to use, please follow their documentation here: &lt;a href=&#34;https://oauth2-proxy.github.io/oauth2-proxy/auth-configuration#azure-auth-provider&#34;&gt;Azure Auth provider&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;You&amp;rsquo;ll notice the Redis connection config.  Please read &lt;a href=&#34;https://oauth2-proxy.github.io/oauth2-proxy/configuration#configuring-for-use-with-the-nginx-auth_request-directive&#34;&gt;this&lt;/a&gt; section of the docs carefully, particularly if you&amp;rsquo;re using Azure authentication.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The key is really the &lt;code&gt;set-xauthrequest&lt;/code&gt; config.  As per the documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;set X-Auth-Request-User, X-Auth-Request-Email and X-Auth-Request-Preferred-Username response headers (useful in Nginx auth_request mode)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So when this is enabled, OAuth2 Proxy will return those 3 headers to the nginx subrequest, making their values available to nginx, rather than just returning an HTTP 2xx without them.&lt;/p&gt;
&lt;p&gt;Is this specific case, we&amp;rsquo;re after the &lt;code&gt;X-Auth-Request-Email&lt;/code&gt; which is returned from Azure AD with the users email address, assuming successful authentication.&lt;/p&gt;
&lt;p&gt;We then need to get that header value (&lt;a href=&#34;mailto:some.users@example.com&#34;&gt;some.users@example.com&lt;/a&gt;) into the &lt;code&gt;es-security-runas-user&lt;/code&gt; and pass that to the upstream Kibana instance.&lt;/p&gt;
&lt;p&gt;Nginx Ingress Resources&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: kibana-ingress
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-Email
      nginx.ingress.kubernetes.io/auth-url: &amp;quot;https://$host/oauth2/auth&amp;quot;
      nginx.ingress.kubernetes.io/auth-signin: &amp;quot;https://$host/oauth2/start?rd=$escaped_request_uri&amp;quot;

      nginx.ingress.kubernetes.io/configuration-snippet: |
        proxy_set_header &#39;es-security-runas-user&#39; $authHeader0;
        proxy_set_header Authorization &amp;quot;Basic &amp;lt;your basic auth string&amp;gt;&amp;quot;;
  spec:
    rules:
    - host: kibana.example.com
      http:
        paths:
        - path: /
          backend:
            serviceName: kibana
            servicePort: 5601

- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: kibana-oauth2-ingress
    annotations:
      kubernetes.io/ingress.class: nginx
  spec:
    rules:
    - host: kibana.example.com
      http:
        paths:
        - path: /oauth2
          backend:
            serviceName: oauth2-proxy
            servicePort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You&amp;rsquo;ll notice there&amp;rsquo;s actually 2 ingresses defined here, for the same hostname, but different paths.  I&amp;rsquo;m not sure this is required, but it appears to be based on my testing.  If you attempt to use a single host, with separate paths for Kibana and OAuth2 Proxy it simply doesnt work.&lt;/p&gt;
&lt;p&gt;In the Nginx Ingress configs, the important pieces are as follows.&lt;/p&gt;
&lt;h1 id=&#34;part-one&#34;&gt;Part One&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;nginx.ingress.kubernetes.io/auth-url: &amp;ldquo;https://$host/oauth2/auth&amp;rdquo;
nginx.ingress.kubernetes.io/auth-signin: &amp;ldquo;https://$host/oauth2/start?rd=$escaped_request_uri&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;code&gt;auth-url&lt;/code&gt; and &lt;code&gt;auth-signin&lt;/code&gt; annotations activate the &lt;code&gt;ngx_http_auth_request_module&lt;/code&gt;, so that every request through this location ( &lt;code&gt;/&lt;/code&gt; ) must be authenticated by the external source specified.  This source is &lt;code&gt;$host/oauth2&lt;/code&gt;, which is the same hostname as kibana, but on the &lt;code&gt;oauth2&lt;/code&gt; path, so its the second of the ingress resources specified above.&lt;/p&gt;
&lt;h1 id=&#34;part-two&#34;&gt;Part Two&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-Email&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;code&gt;auth-response-headers&lt;/code&gt; annotation simply passes the &lt;code&gt;X-Auth-Request-Email&lt;/code&gt; header that we received from Oauth2 Proxy onto the upstream, Kibana, assuming successful authentication.&lt;/p&gt;
&lt;p&gt;Relevant documentation for the annotation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;nginx.ingress.kubernetes.io/auth-response-headers: &amp;lt;Response_Header_1, &amp;hellip;, Response_Header_n&amp;gt; to specify headers to pass to backend once authentication request completes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This by itself doesn&amp;rsquo;t help much, as Kibana has no idea to do anything with that specific header, but the trick is that the ingress controller does this by setting an nginx var to the value of that header as returned by Oauth2 Proxy, and then setting the same header to be passed upstream using &lt;code&gt;proxy_set_header&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Relevant nginx config snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;auth_request_set $authHeader0 $upstream_http_x_auth_request_email;
proxy_set_header &#39;X-Auth-Request-Email&#39; $authHeader0;
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;part-three&#34;&gt;Part Three&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;proxy_set_header &amp;lsquo;es-security-runas-user&amp;rsquo; $authHeader0;
proxy_set_header Authorization &amp;ldquo;Basic &lt;your basic auth string&gt;&amp;quot;;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The important piece for Kibana authentication comes next, by re-using the nginx var &lt;code&gt;$authHeader0&lt;/code&gt;, creating and setting the &lt;code&gt;es-security-runas-user&lt;/code&gt; header using that var, and passing that upstream using &lt;code&gt;proxy_set_header&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As long as Kibana receives the &lt;code&gt;es-security-runas-user&lt;/code&gt; and the basic auth header &lt;code&gt;Authorization: Basic &amp;lt;your basic auth string&amp;gt;&lt;/code&gt;, it will attempt to &amp;ldquo;login&amp;rdquo; using the value of the &lt;code&gt;es-security-runas-user&lt;/code&gt; header.&lt;/p&gt;
&lt;p&gt;In this case we&amp;rsquo;re using emails as the username, but if your auth source provides a different value that OAuth2 Proxy can return to nginx, the same approach could be used.&lt;/p&gt;
&lt;p&gt;Since this isintegrated with the Azure AD authentication flow, it can also take advantage of other authentication requirements, such as requiring MFA, or even Azure AD&amp;rsquo;s Conditional Access system.  These sorts of features are available on most other auth providers I&amp;rsquo;ve seen as well of course.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Puppet Module for FoundationDB</title>
      <link>/posts/puppet-module-for-foundationdb/</link>
      <pubDate>Tue, 18 Nov 2014 16:30:00 +0000</pubDate>
      
      <guid>/posts/puppet-module-for-foundationdb/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;FoundationDB has been purchased by Apple, and is no longer a product. Apple will be cancelling all support contracts and apprently not continuing to offer FDB for sale. Therefore this module is of course deprecated and no longer supported (at least by me!).&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Here at Bede we&amp;rsquo;ve been using &lt;a href=&#34;https://foundationdb.com/&#34;&gt;FoundationDB&lt;/a&gt; for some time, and are basically in love with it.  Fully distributed, ACID compliant transactions?  Yes please.  It&amp;rsquo;s fast, it scales, it&amp;rsquo;s reliable and redundant.  From an operations persons point of view it&amp;rsquo;s simply sexy.&lt;/p&gt;
&lt;p&gt;However, it is a relatively new technology, with a small but growing community.  That means that the ecosystem surrounding FDB is not quite as extensive as many of the other, longer lived persistence technologies.  Therefore there was no Puppet modules available to manage installation and configuration, so we had to write our own.&lt;/p&gt;
&lt;p&gt;The &lt;a href=&#34;https://github.com/BedeGaming/puppet-foundationdb&#34;&gt;module&lt;/a&gt; can manage installation, configuration, upgrading, etc.  All the usual bits&amp;rsquo;n&amp;rsquo;bobs you&amp;rsquo;d expect a Puppet module to manage.  This does come with one caveat however:  it doesn&amp;rsquo;t currently manage FDB cluster membership, due to the way FDB itself manages membership.  We&amp;rsquo;re still exploring ways to add at least some amount of membership management into the module, and of course if anyone wants to contribute towards the module, feel free to fork and issue PR&amp;rsquo;s.&lt;/p&gt;
&lt;p&gt;The FoundationDB module can be found on the &lt;a href=&#34;https://forge.puppetlabs.com/bedegaming/foundationdb&#34;&gt;Forge&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Modelling an nginx Application Proxy using Puppet and Hiera</title>
      <link>/posts/modelling-an-nginx-application-proxy-using-puppet-and-hiera/</link>
      <pubDate>Wed, 05 Nov 2014 15:53:08 +0000</pubDate>
      
      <guid>/posts/modelling-an-nginx-application-proxy-using-puppet-and-hiera/</guid>
      <description>&lt;h1 id=&#34;goal&#34;&gt;Goal&lt;/h1&gt;
&lt;p&gt;To model nginx configurations in Hiera, extracting those configurations from the Puppet DSL, and create a Puppet Profile to combine that with the nginx module.&lt;/p&gt;
&lt;!-- more --&gt;
&lt;h1 id=&#34;links&#34;&gt;Links&lt;/h1&gt;
&lt;h2 id=&#34;puppet-docs&#34;&gt;Puppet Docs&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/learning/ral.html&#34;&gt;Puppet Resource Abstraction Layer&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/references/latest/function.html#createresources&#34;&gt;Puppet create_resources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/guides/scope_and_puppet.html&#34;&gt;Scoping&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/puppet/latest/reference/lang_namespaces.html&#34;&gt;Namespacing&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;puppet-rolesprofiles&#34;&gt;Puppet Roles/Profiles&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://www.slideshare.net/PuppetLabs/roles-talk&#34;&gt;Craig Dunn, Designing Puppet Talk&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://ask.puppetlabs.com/question/1655/an-end-to-end-roleprofile-example-using-hiera/&#34;&gt;End to End Roles and Profiles Example&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://garylarizza.com/blog/2014/02/17/puppet-workflow-part-1/&#34;&gt;Puppet Workflow Part 1&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;http://garylarizza.com/blog/2014/02/17/puppet-workflow-part-2/&#34;&gt;Puppet Workflow Part 2&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;hiera&#34;&gt;Hiera&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/hiera/1/&#34;&gt;Puppet Labs Hiera Docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/hiera/1/variables.html#passing-variables-to-hiera&#34;&gt;Passing Variables to Hiera&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;nginx-module&#34;&gt;nginx Module&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/jfryman/puppet-nginx&#34;&gt;jfryman nginx Puppet Module&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;nginx&#34;&gt;nginx&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://wiki.nginx.org/Main&#34;&gt;nginx Project Page&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;requirements&#34;&gt;Requirements&lt;/h1&gt;
&lt;p&gt;Within nginx we need to define a set of nginx &amp;ldquo;server&amp;rdquo; contexts with the
same set of &amp;ldquo;location&amp;rdquo; contexts in each &amp;ldquo;server&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Think of this as &amp;ldquo;&lt;a href=&#34;http://SiteA.com&#34;&gt;http://SiteA.com&lt;/a&gt;&amp;rdquo; and &amp;ldquo;&lt;a href=&#34;http://SiteB.com&#34;&gt;http://SiteB.com&lt;/a&gt;&amp;rdquo;, that both
have the same set of &amp;ldquo;locations&amp;rdquo;, which define the nginx upstreams
(backends), that combined provide a single site that is created from
disparate SOA applications.&lt;/p&gt;
&lt;p&gt;This is essentially a model to build a nginx based proxy layer sitting
in front of a multi-tenant SOA application stack.&lt;/p&gt;
&lt;h2 id=&#34;approach&#34;&gt;Approach&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;Example 1: Single Server, Two Locations modelled in Hiera (YAML)&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;proxy::sites::vhosts:
  SiteA:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false

proxy::sites::locations:
  root:
    vhost : SiteA
    location : &#39;/&#39;
    ensure : &#39;present&#39;
    proxy : &#39;http://serviceA&#39;
  server_status: 
    vhost : SiteA
    location : &#39;/server-status&#39;
    ensure : &#39;present&#39;
    stub_status : true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In this example we have a single &amp;ldquo;server&amp;rdquo;, called SiteA. SiteA also has 2 locations defined, &amp;ldquo;root&amp;rdquo;, and &amp;ldquo;server-status&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The relationship between the server and location is defined by the parameter &amp;ldquo;vhost&amp;rdquo; in the location stanza; by being set to &amp;ldquo;SiteA&amp;rdquo;, the location stanza is created within the SiteA server stanza, when all this is realized into an actual nginx configuration file.&lt;/p&gt;
&lt;p&gt;There are possibly other Hiera/YAML structures that would allow us to
model this relationship more clearly, however we&amp;rsquo;re using this specific
structure because we want to take advantage of Puppets
&lt;code&gt;create_resources&lt;/code&gt; function in conjunction with the nginx module:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/references/latest/function.html#createresources&#34;&gt;https://docs.puppetlabs.com/references/latest/function.html#createresources&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Essentially, with the data structured in the correct manner we can pass
it to &lt;code&gt;create_resources&lt;/code&gt; as a Puppet hash and it will create the
specified resource. In this case the specific resources are the location
and vhost resources defined in the nginx module. By defining our config
data in Hiera in the right structure, we can use &lt;code&gt;create_resources&lt;/code&gt; and
make our life a bit simpler.&lt;/p&gt;
&lt;p&gt;This presents a challenge when you want to create more than one server
(multiple vhosts).&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 2: Two vhosts, Two locations&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;proxy::sites::vhosts:
  SiteA:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false
  SiteB:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false

proxy::sites::locations:
  root:
    vhost : SiteA
    location : &#39;/&#39;
    ensure : &#39;present&#39;
    proxy : &#39;http://serviceA
  server_status:
    vhost : SiteA
    location : &#39;/server-status&#39;
    ensure : &#39;present&#39;
    stub_status : true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You now have 2 servers/vhosts. However the location stanzas are only
related to the first server/vhost (SiteA), as specified by the locations
vhost paramater.&lt;/p&gt;
&lt;p&gt;In order to create a &amp;ldquo;root&amp;rdquo; and &amp;ldquo;server_status&amp;rdquo; location for SiteB,
following the same model as above, you would end up with:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 3: Two vhosts, Four locations (2 per vhost)&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;proxy::sites::vhosts:
  SiteA:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false
  SiteB:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false

proxy::sites::locations:
  SiteA_root:
    vhost : SiteA
    location : &#39;/&#39;
    ensure : &#39;present&#39;
    proxy : &#39;http://serviceA
  SiteA_server_status:
    vhost : SiteA
    location : &#39;/server-status&#39;
    ensure : &#39;present&#39;
    stub_status : true
  SiteB_root:
    vhost : SiteB
    location : &#39;/&#39;
    ensure : &#39;present&#39;
    proxy : &#39;http://serviceA
  SiteB_server_status:
    vhost : SiteB
    location : &#39;/server-status&#39;
    ensure : &#39;present&#39;
    stub_status : true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This is obviously suboptimal as you&amp;rsquo;re repeating configuration for both
locations now. For each vhost SiteA and SiteB, you need to define 2
locations.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll also notice that the name of the locations have now changed to
&amp;ldquo;SiteA_root&amp;rdquo;, &amp;ldquo;SiteA_server_status&amp;rdquo; and &amp;ldquo;SiteB_root&amp;rdquo;,
&amp;ldquo;SiteB_server_status&amp;rdquo;. This is due to Puppet not allowing duplicate
resource declarations, which these locations would be if they were both
named &amp;ldquo;root&amp;rdquo; and &amp;ldquo;server_status&amp;rdquo;. So we create unique resources, by
making the resource names unique.&lt;/p&gt;
&lt;p&gt;You can see where this is leading. For every vhost that we want to add,
that has the same set of locations, we&amp;rsquo;ll end up duplicating the
location data, and just changing the location name and the vhost the
location is associated with. This is probably bad, at least it feels
very bad to me.&lt;/p&gt;
&lt;p&gt;The natural response to this is &amp;ldquo;Simples! I&amp;rsquo;ll just iterate over the
list of vhosts and create the set of locations for each server!&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Nope. Not really.&lt;/p&gt;
&lt;p&gt;Puppet DSL doesn&amp;rsquo;t really do iteration. Not naturally anyhow.&lt;/p&gt;
&lt;p&gt;My favorite comment on this:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;http://stackoverflow.com/questions/12958114/how-to-iterate-over-an-array-in-puppet/13008766#13008766&#34;&gt;http://stackoverflow.com/questions/12958114/how-to-iterate-over-an-array-in-puppet/13008766#13008766&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Quote: &amp;ldquo;The Puppet developers have irrational prejudices against
iteration based on a misunderstanding about how declarative languages
work.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;(Disclaimer: I&amp;rsquo;m not a dev, I won&amp;rsquo;t attempt to evaluate the truth of
that statement, but I still think its amusing)&lt;/p&gt;
&lt;p&gt;So we have 2 problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Iteration in Puppet is not normal, allowed, right, easy?&lt;/li&gt;
&lt;li&gt;The &amp;ldquo;names&amp;rdquo; of the locations need to be unique, which means they
can&amp;rsquo;t simply be text labels in YAML. There must be some
interpolation somewhere.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;We&amp;rsquo;ll deal with the second problem first, as it&amp;rsquo;s a slightly easier
problem to solve, at least in my experience.&lt;/p&gt;
&lt;h2 id=&#34;hiera-interpolation&#34;&gt;Hiera Interpolation&lt;/h2&gt;
&lt;p&gt;Hiera/Puppet allows variables to be passed into Hiera:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/hiera/1/variables.html#passing-variables-to-hiera&#34;&gt;https://docs.puppetlabs.com/hiera/1/variables.html#passing-variables-to-hiera&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Quote: &amp;ldquo;When used with Puppet, Hiera automatically receives all of
Puppet&amp;rsquo;s current variables. This includes facts and built-in variables,
as well as local variables from the current scope&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;This means we can pass Hiera some variable defined within the Puppet
Manifest, and hopefully therereby creating dynamically named nginx
location resources within Puppet. Probably the most important bit of
that quote is that Hiera has access to &amp;ldquo;local variables from the current
scope&amp;rdquo;. That&amp;rsquo;ll be touched upon further on in this article.&lt;/p&gt;
&lt;p&gt;So, that would look something like Example 4.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 4: Two vhosts, 2 dynamically named locations&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;proxy::sites::vhosts:
  SiteA:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false
  SiteB:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false

proxy::sites::locations:
  &amp;quot;%{vhost}_root&amp;quot;:
    vhost : &amp;quot;%{vhost}&amp;quot;
    location : &#39;/&#39;
    ensure : &#39;present&#39;
    proxy : &#39;http://serviceA
  &amp;quot;%{vhost}_server_status&amp;quot;:
    vhost : &amp;quot;%{vhost}&amp;quot;
    location : &#39;/server-status&#39;
    ensure : &#39;present&#39;
    stub_status : true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So, without getting into the intricate details, this suggests that the
nginx location resources will be named &lt;code&gt;&amp;lt;vhost&amp;gt;_&amp;lt;location&amp;gt;&lt;/code&gt;. The goal
would be to end up with 4 locations, 2 assigned to each vhost.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 5: Locations per Vhost&lt;/em&gt;&lt;/p&gt;
&lt;table style=&#34;width:100%&#34;&gt;
  &lt;tr&gt;
    &lt;th&gt;Vhost&lt;/td&gt;
    &lt;th&gt;Location Name&lt;/td&gt; 
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SiteA&lt;/td&gt;
    &lt;td&gt;SiteA_root&lt;/td&gt; 
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;SiteA_server_status&lt;/td&gt; 
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;SiteB&lt;/td&gt;
    &lt;td&gt;SiteB_root&lt;/td&gt; 
  &lt;/tr&gt;
  &lt;tr&gt;
    &lt;td&gt;&lt;/td&gt;
    &lt;td&gt;SiteB_server_status	&lt;/td&gt; 
  &lt;/tr&gt;
&lt;/table&gt;
&lt;p&gt;The next problem to solve is how to iterate over the list of vhosts.&lt;/p&gt;
&lt;h2 id=&#34;puppet-iteration&#34;&gt;Puppet Iteration&lt;/h2&gt;
&lt;p&gt;As mentioned, Puppet doesn&amp;rsquo;t really do iteration natively (ignoring the
future parser for now). So how do we accomplish this using native Puppet
DSL?&lt;/p&gt;
&lt;p&gt;The first thing to realize is that iteration does in fact happen (or
something similar enough that the difference doesnt matter to most
people). A very good and clear blog article about this can be found
here:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://tobrunet.ch/2013/01/iterate-over-datastructures-in-puppet-manifests/&#34;&gt;https://tobrunet.ch/2013/01/iterate-over-datastructures-in-puppet-manifests/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The basic point being made is that native Puppet functions will iterate
over a Puppet array, executing for each array member.&lt;/p&gt;
&lt;p&gt;The first example on that blog post is as follows.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 6: Iteration in Puppet DSL&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;define arrayDebug {
 notify { [Item ${name}]() }
}

class array {
 $array = [ &#39;item1&#39;, &#39;item2&#39;, &#39;item3&#39; ]
 arrayDebug { $array: }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Essentially the &lt;code&gt;arrayDebug&lt;/code&gt; resource (define) will iterate over the
array members of the array &lt;code&gt;$array&lt;/code&gt;.&lt;/p&gt;
&lt;h2 id=&#34;creating-a-puppet-profile&#34;&gt;Creating a Puppet Profile&lt;/h2&gt;
&lt;p&gt;So the next step is to write a Puppet &amp;ldquo;Profile&amp;rdquo; for nginx that
encapsulates the functionality we want, using Hiera as the data source,
and the nginx module as the underlying component.&lt;/p&gt;
&lt;p&gt;Lets start by creating the basic Hiera config needed for a very simple
nginx install.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 7: Hiera for nginx&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;nginx::default::mail : false
nginx::default::worker_processes : %{processorcount}
nginx::default::server_tokens : &#39;off&#39;
nginx::default::nginx_error_log : &#39;/var/log/nginx/error.log debug&#39;
nginx::default::http_access_log : &#39;/var/log/nginx/access.log&#39;
nginx::default::proxy_cache_path : &#39;/var/cache/nginx&#39;
nginx::default::proxy_cache_levels : &#39;2&#39;
nginx::default::proxy_cache_keys_zone : &#39;cache:10m&#39;
nginx::default::proxy_cache_max_size : &#39;2048m&#39;
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;There&amp;rsquo;s nothing exceptional here, this is all nginx boilerplate to set
some global defaults for nginx. The interesting bit of this is when we
start creating the nginx Profile.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 8: Base nginx Profile in Puppet DSL&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class profile::linux::nginx {

  $mail							= hiera(&#39;nginx::default::mail&#39;)
  $worker_processes				= hiera(&#39;nginx::default::worker_processes&#39;)
  $server_tokens				= hiera(&#39;nginx::default::server_tokens&#39;)
  $nginx_error_log				= hiera(&#39;nginx::default::nginx_error_log&#39;)
  $http_access_log				= hiera(&#39;nginx::default::http_access_log&#39;)
  $proxy_cache_path				= hiera(&#39;nginx::default::proxy_cache_path&#39;)
  $proxy_cache_levels			= hiera(&#39;nginx::default::proxy_cache_levels&#39;)
  $proxy_cache_keys_zone		= hiera(&#39;nginx::default::proxy_cache_keys_zone&#39;)
  $proxy_cache_max_size			= hiera(&#39;nginx::default::proxy_cache_max_size&#39;)
  $names_hash_bucket_size		= hiera(&#39;nginx::default::names_hash_bucket_size&#39;)

  #################################################
  # Create the base nginx config by passing the nginx module the minimum config that we use across all our systems
  #################################################

  class { &#39;::nginx&#39;:
    mail						=&amp;gt; $mail,
    worker_processes			=&amp;gt; $worker_processes,
    server_tokens 				=&amp;gt; $server_tokens,
    nginx_error_log 			=&amp;gt; $nginx_error_log,
    http_access_log 			=&amp;gt; $http_access_log,
    proxy_cache_path 			=&amp;gt; $proxy_cache_path,
    proxy_cache_levels 			=&amp;gt; $proxy_cache_levels,
    proxy_cache_keys_zone 		=&amp;gt; $proxy_cache_keys_zone,
    proxy_cache_max_size 		=&amp;gt; $proxy_cache_max_size,
    names_hash_bucket_size 		=&amp;gt; $names_hash_bucket_size,
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So this is all pretty simple again. This is a Puppet class called
&lt;code&gt;profile::linux::nginx&lt;/code&gt;, which sets some default variables, and passes
them to another class called &lt;code&gt;::nginx&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;This class is a &amp;ldquo;Profile&amp;rdquo; simply by dint of it being created within the
Profile module / namespace. It&amp;rsquo;s technically no different than any other
Puppet module, it&amp;rsquo;s simply a widely accepted convention that Puppet
Roles and Profiles are modules.&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;::nginx&lt;/code&gt; class actually refers to the nginx module, denoted by the
leading &lt;code&gt;::&lt;/code&gt;, basically meaning &amp;ldquo;at top scope, named nginx&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;If you&amp;rsquo;re wondering about the &lt;code&gt;::&lt;/code&gt; colon syntax, start by reading about
Puppet scope and namespacing:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/guides/scope_and_puppet.html&#34;&gt;https://docs.puppetlabs.com/guides/scope_and_puppet.html&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/puppet/latest/reference/lang_namespaces.html&#34;&gt;https://docs.puppetlabs.com/puppet/latest/reference/lang_namespaces.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So this nginx Profile achieves the very first of our goals; that is to
install nginx and set some very basic global nginx parameters.&lt;/p&gt;
&lt;p&gt;The next step would be create the nginx &amp;ldquo;servers&amp;rdquo; (called vhosts here).
In order to achieve this step we need to retrieve the vhost
configuration data from Hiera. We&amp;rsquo;ll then use that data to create the
vhosts.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 9: nginx Profile with vhosts being created&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class profile::linux::nginx {

  $mail							= hiera(&#39;nginx::default::mail&#39;)
  $worker_processes				= hiera(&#39;nginx::default::worker_processes&#39;)
  $server_tokens				= hiera(&#39;nginx::default::server_tokens&#39;)
  $nginx_error_log				= hiera(&#39;nginx::default::nginx_error_log&#39;)
  $http_access_log				= hiera(&#39;nginx::default::http_access_log&#39;)
  $proxy_cache_path				= hiera(&#39;nginx::default::proxy_cache_path&#39;)
  $proxy_cache_levels			= hiera(&#39;nginx::default::proxy_cache_levels&#39;)
  $proxy_cache_keys_zone		= hiera(&#39;nginx::default::proxy_cache_keys_zone&#39;)
  $proxy_cache_max_size			= hiera(&#39;nginx::default::proxy_cache_max_size&#39;)
  $names_hash_bucket_size		= hiera(&#39;nginx::default::names_hash_bucket_size&#39;)

  #################################################
  # Get the hash of vhosts from Hiera. Extract the hash keys into a list (array).
  #################################################

  $vhosts						= hiera(&#39;proxy::sites::vhosts&#39;)

  #################################################
  # Create the base nginx config by passing the nginx module the minimum config that we use across all our systems
  #################################################

  class { &#39;::nginx&#39;:
    mail						=&amp;gt; $mail,
    worker_processes			=&amp;gt; $worker_processes,
    server_tokens 				=&amp;gt; $server_tokens,
    nginx_error_log 			=&amp;gt; $nginx_error_log,
    http_access_log 			=&amp;gt; $http_access_log,
    proxy_cache_path 			=&amp;gt; $proxy_cache_path,
    proxy_cache_levels 			=&amp;gt; $proxy_cache_levels,
    proxy_cache_keys_zone 		=&amp;gt; $proxy_cache_keys_zone,
    proxy_cache_max_size 		=&amp;gt; $proxy_cache_max_size,
    names_hash_bucket_size 		=&amp;gt; $names_hash_bucket_size,
  }

  ################################################
  # Create the ngingx vhosts (server blocks in nginx config terms). Pass create_resources the hash retrieved from Hiera,
  # into the nginx::resource::vhost resource defined by the nginx module
  ################################################

  create_resources(&#39;::nginx::resource::vhost&#39;, $vhosts)

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;First we add some functionality to retrieve the vhost config from Hiera
using a Hiera lookup function in Puppet:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$vhosts = hiera(&#39;proxy::walletv2::vhosts&#39;)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We then pass the resultant hash to a Puppet function called
&lt;code&gt;create_resrouces&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;create_resources(&#39;::nginx::resource::vhost&#39;, $vhosts)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;create_resources&lt;/code&gt; is a very useful Puppet function. Given the right set
of information, in the right structure, it will auto-magically create
the specified resource. In this case the resource we&amp;rsquo;re talking about is
the &lt;code&gt;::nginx::resource::vhost&lt;/code&gt; resource. This resource is defined in the
nginx module, as denoted by the top scope &lt;code&gt;::nginx&lt;/code&gt;. The hash &lt;code&gt;$vhosts&lt;/code&gt;
contains the data retrieved from Hiera, specifically from the key
&lt;code&gt;proxy::sites::vhosts&lt;/code&gt; found in Example 2 to Example 4.&lt;/p&gt;
&lt;p&gt;This will create the nginx vhost (server) configurations in the actual
nginx config files, using the key/values as parameters.&lt;/p&gt;
&lt;p&gt;Further reading:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/references/latest/function.html#createresources&#34;&gt;https://docs.puppetlabs.com/references/latest/function.html#createresources&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://docs.puppetlabs.com/learning/ral.html&#34;&gt;https://docs.puppetlabs.com/learning/ral.html&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The final step is to combine our interpolated Hiera data and Puppet
iteration learnings into the Profile so that each vhost that gets
created, also gets the same set of locations.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 10: nginx Profile with vhosts and locations being created&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;class profile::linux::nginx {

  $mail							= hiera(&#39;nginx::default::mail&#39;)
  $worker_processes				= hiera(&#39;nginx::default::worker_processes&#39;)
  $server_tokens				= hiera(&#39;nginx::default::server_tokens&#39;)
  $nginx_error_log				= hiera(&#39;nginx::default::nginx_error_log&#39;)
  $http_access_log				= hiera(&#39;nginx::default::http_access_log&#39;)
  $proxy_cache_path				= hiera(&#39;nginx::default::proxy_cache_path&#39;)
  $proxy_cache_levels			= hiera(&#39;nginx::default::proxy_cache_levels&#39;)
  $proxy_cache_keys_zone		= hiera(&#39;nginx::default::proxy_cache_keys_zone&#39;)
  $proxy_cache_max_size			= hiera(&#39;nginx::default::proxy_cache_max_size&#39;)
  $names_hash_bucket_size		= hiera(&#39;nginx::default::names_hash_bucket_size&#39;)

  #################################################
  # Get the hash of vhosts from Hiera. Extract the hash keys into a list (array).
  #################################################

  $vhosts						= hiera(&#39;proxy::sites::vhosts&#39;)
  $vhostslist					= keys($vhosts)

  #################################################
  # Create the base nginx config by passing the nginx module the minimum config that we use across all our systems
  #################################################

  class { &#39;::nginx&#39;:
    mail						=&amp;gt; $mail,
    worker_processes			=&amp;gt; $worker_processes,
    server_tokens 				=&amp;gt; $server_tokens,
    nginx_error_log 			=&amp;gt; $nginx_error_log,
    http_access_log 			=&amp;gt; $http_access_log,
    proxy_cache_path 			=&amp;gt; $proxy_cache_path,
    proxy_cache_levels 			=&amp;gt; $proxy_cache_levels,
    proxy_cache_keys_zone 		=&amp;gt; $proxy_cache_keys_zone,
    proxy_cache_max_size 		=&amp;gt; $proxy_cache_max_size,
    names_hash_bucket_size 		=&amp;gt; $names_hash_bucket_size,
  }

  ################################################
  # Create the ngingx vhosts (server blocks in nginx config terms). Pass create_resources the hash retrieved from Hiera,
  # into the nginx::resource::vhost resource defined by the nginx module
  ################################################

  create_resources(&#39;::nginx::resource::vhost&#39;, $vhosts)

  define profile::linux::nginx::location {
    ### Creating Locations
    $locations = hiera(&#39;proxy::sites::locations&#39;)
    $vhostslocations = { vhost =&amp;gt; $name }

    create_resources(&#39;::nginx::resource::location&#39;, $locations, $vhostslocations)
  }

  ################################################
  #
  # This calls the above define per member of $vhostslist array
  #
  ################################################

  profile::linux::nginx::location { $vhostslist: }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So what have we done here?&lt;/p&gt;
&lt;p&gt;The first thing is that we&amp;rsquo;ve added an array called &lt;code&gt;$vhostslist&lt;/code&gt;, whos
array members are made up of the key names of the &lt;code&gt;$vhosts&lt;/code&gt; hash:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;$vhostslist = keys($vhosts)
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This gives us the array that we want to iterate over, as discussed in
the Puppet iteration section. The array members in this example will be
&amp;ldquo;SiteA&amp;rdquo; and &amp;ldquo;SiteB&amp;rdquo;, i.e. &lt;code&gt;[ &amp;quot;SiteA&amp;quot;, &amp;quot;SiteB&amp;quot; ]&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The second thing we&amp;rsquo;ve added is a new define,
&lt;code&gt;profile::linux::nginx::location&lt;/code&gt;:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;define profile::linux::nginx::location {
  ### Creating Locations
  $locations = hiera(&#39;proxy::sites::locations&#39;)
  $vhostslocations = { vhost =&amp;gt; $name }

  create_resources(&#39;::nginx::resource::location&#39;, $locations, $vhostslocations)
 }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;This can be thought of as a function in more general programming
languages, that will iterate over the array &lt;code&gt;$vhostslist&lt;/code&gt;. Within this
define we are retrieving the list of locations into a has called
&lt;code&gt;$locations&lt;/code&gt;, using another Hiera lookup, on the key
&lt;code&gt;proxy::sites::locations&lt;/code&gt; (Yes I&amp;rsquo;m aware this will be done for every
iteration, not optimal). We then create a new hash called
&lt;code&gt;$vhostslocations&lt;/code&gt; which contains the key &lt;code&gt;vhost&lt;/code&gt; and value &lt;code&gt;$name&lt;/code&gt;,
i.e. &lt;code&gt;{ vhost =&amp;gt; $name }&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Once we have the &lt;code&gt;$locations&lt;/code&gt; hash and the &lt;code&gt;$vhostslocations&lt;/code&gt; hash, we
then call &lt;code&gt;create_resources&lt;/code&gt;. Just like the previous time we called
&lt;code&gt;create_resources&lt;/code&gt;, the specific resource is defined by the nginx
module, and is called &lt;code&gt;::nginx::resource::location&lt;/code&gt;. However, in this
case we&amp;rsquo;re giving the &lt;code&gt;create_resources&lt;/code&gt; function two items, not one.
The first item is the list of locations we&amp;rsquo;d like to create, and the
second is the &lt;code&gt;$vhostslocations&lt;/code&gt; hash.&lt;/p&gt;
&lt;p&gt;The second optional item passed to &lt;code&gt;create_resources&lt;/code&gt; (
&lt;code&gt;$vhostslocations&lt;/code&gt; ) is an anonymous hash of extra parameters to add to
the resource being created.&lt;/p&gt;
&lt;p&gt;This define is then &amp;ldquo;called&amp;rdquo; by the line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;profile::linux::nginx::location { $vhostslist: }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we go further, we need to look at the Hiera data again, and
modify it slightly:&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Example 11: Modified Hiera data&lt;/em&gt;&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;proxy::sites::vhosts:
  SiteA:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false
  SiteB:
    listen_port : 8080
    rewrite_to_https : false
    use_default_location : false

proxy::sites::locations:
  &amp;quot;%{name}_root&amp;quot;
    location : &#39;/&#39;
    ensure : &#39;present&#39;
    proxy : &#39;http://serviceA&#39;
  &amp;quot;%{name}_server_status&amp;quot;
    location : &#39;/server-status&#39;
    ensure : &#39;present&#39;
    stub_status : true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The change here is the variable we want interpolated has been changed to
&lt;code&gt;%{name}&lt;/code&gt;, and the &lt;code&gt;vhost&lt;/code&gt; parameter has been removed.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll notice in Example 10 the newly added define suddenly makes use of
a variable called &lt;code&gt;$name&lt;/code&gt;. We&amp;rsquo;re also now referencing it in our modified
Hiera config in Example 11.&lt;/p&gt;
&lt;p&gt;So what is &lt;code&gt;$name&lt;/code&gt;? Well, this goes back to Puppet resources again. The
define &lt;code&gt;profile::linux::nginx::location&lt;/code&gt; &lt;strong&gt;is&lt;/strong&gt; a Puppet resource, and
as such must be &amp;ldquo;named&amp;rdquo; (and again, named uniquely). When we call the
define:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;profile::linux::nginx::location { $vhostslist: }
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We&amp;rsquo;re passing it the array that it iterates over, but under the hood in
a bit of Puppet magic, the &amp;ldquo;name&amp;rdquo; of the resource becomes the value of
the array member. So there will be two resources of type
&lt;code&gt;profile::linux::nginx::location&lt;/code&gt;, one named &amp;ldquo;SiteA&amp;rdquo; and the other named
&amp;ldquo;SiteB&amp;rdquo;. Within the scope of each of these resources, Puppet creates and
makes available a variable called &lt;code&gt;$name&lt;/code&gt;, which of course is set to the
&amp;ldquo;name&amp;rdquo; of the resource, or in our case &amp;ldquo;SiteA&amp;rdquo; and SiteB&amp;quot;.&lt;/p&gt;
&lt;p&gt;Since we are performing a Hiera lookup within the resource (define)
&lt;code&gt;profile::linux::nginx::location&lt;/code&gt;, that Hiera lookup now has access to
all the Puppet variables within that scope (see previous info about
Hiera being passed Puppet variables). Therefore the &lt;code&gt;%{name}&lt;/code&gt; Hiera
variable is set to the &lt;code&gt;$name&lt;/code&gt; Puppet variable. Magic.&lt;/p&gt;
&lt;p&gt;The final piece of the puzzle is the &lt;code&gt;$vhostslocations&lt;/code&gt; hash, and the
actual use of the &lt;code&gt;$name&lt;/code&gt; variable. According to the &lt;code&gt;create_resources&lt;/code&gt;
documentation:&lt;/p&gt;
&lt;p&gt;&amp;ldquo;The values given on the third argument are added to the parameters of
each resource present in the set given on the second argument.&amp;rdquo;&lt;/p&gt;
&lt;p&gt;So therefore the hash &lt;code&gt;$vhostslocations { vhost =&amp;gt; &amp;quot;$name&amp;quot; }&lt;/code&gt; basically
gives the nginx module an extra parameter that is applied to each
location created of &amp;ldquo;vhost =&amp;gt; SiteA&amp;rdquo; or &amp;ldquo;vhost =&amp;gt; SiteB&amp;rdquo;. This
establishes the relationship between the locations and their parent
vhosts.&lt;/p&gt;
&lt;p&gt;Put it all together and we now have a start towards cleanly modelling
nginx configurations in Hiera, without repeating data, and using the
Roles and Profile pattern. This allows us to completely extract site
configuration data from the Puppet DSL, by providing an abstraction
layer to the nginx module (the nginx Profile). I didn&amp;rsquo;t actually touch
upon a Puppet Role here, but once you have your Profile(s) created, the
creation of Roles is generally much simpler, as they should be nothing
more than a collection of Profiles. The links provided at the beginning
of this post on Roles/Profiles should make the rest clear.&lt;/p&gt;
&lt;p&gt;So that&amp;rsquo;s basically an end-to-end example (although not necessarily 100%
working) of using Hiera to model nginx, and combining that with an
abstraction layer (the Profile) above the nginx module.&lt;/p&gt;
&lt;h2 id=&#34;possible-re-factors&#34;&gt;Possible re-factors&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;Duplicate data in the vhosts configurations could be reduced using
the same iteration trick used for the location configs.&lt;/li&gt;
&lt;li&gt;Removing the Hiera lookup from the &lt;code&gt;profile::linux::nginx::location&lt;/code&gt;
resource would prevent repeating the lookup.&lt;/li&gt;
&lt;li&gt;Remove the use of the &lt;code&gt;$vhostslocations&lt;/code&gt; hash. Use Hiera
interpolation to specify the vhost parameter in the location
resource.&lt;/li&gt;
&lt;li&gt;Many others I haven&amp;rsquo;t thought of :)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;If anyone has any improvements, suggestions, comments or criticisms let
me know at bedeblog at undertow dot ca.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
