<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>elasticsearch on Home</title>
    <link>/tags/elasticsearch/</link>
    <description>Home (elasticsearch)</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 17 Aug 2021 08:32:41 +0100</lastBuildDate>
    
    <atom:link href="/tags/elasticsearch/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Alerting using SIEM Detections and ElastAlert</title>
      <link>/posts/alerting-using-siem-detections-and-elastalert/</link>
      <pubDate>Tue, 17 Aug 2021 08:32:41 +0100</pubDate>
      
      <guid>/posts/alerting-using-siem-detections-and-elastalert/</guid>
      <description>&lt;p&gt;ElasticSearch SIEM Detections and Alerts and Actions are quite useful features, except for the fact that actual alerting is behind a license paywall.  So while both of these features can run rules, check for conditions, and record the results in an index, neither of them actually provide &lt;em&gt;alerting&lt;/em&gt; support.&lt;/p&gt;
&lt;p&gt;Alerting requires a Gold License, which if alerting is the only thing you want, is an excessive cost.&lt;/p&gt;
&lt;p&gt;If you can&amp;rsquo;t move off ElasticSearch to &lt;a href=&#34;https://opensearch.org/&#34;&gt;OpenSearch&lt;/a&gt;, which has Alerting available for free, you can use tools such as &lt;a href=&#34;https://github.com/jertel/elastalert2&#34;&gt;ElastAlert2&lt;/a&gt;&lt;sup&gt;1&lt;/sup&gt; to handle the Alerting requirements.&lt;/p&gt;
&lt;h1 id=&#34;siem-detections&#34;&gt;SIEM Detections&lt;/h1&gt;
&lt;p&gt;The following example is for SIEM Detections, and alerting with ElastAlert2.&lt;/p&gt;
&lt;p&gt;SIEM Detections record their results in an index called &lt;code&gt;.siem-signales-default&lt;/code&gt;.  The &lt;code&gt;-default&lt;/code&gt; part is based on the Kibana space, so if you&amp;rsquo;re using a Kibana space called &lt;code&gt;exampleA&lt;/code&gt;, the index name would be &lt;code&gt;.siem-signals-examplea&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Make sure to create a Kibana Index Pattern for this index, so you can explore it fully.&lt;/p&gt;
&lt;p&gt;The key field to be aware of is the &lt;code&gt;signal.rule.name&lt;/code&gt;, which is of course the SIEM Detection Rule name.  This is what we&amp;rsquo;ll use to create an ElastAlert rule.&lt;/p&gt;
&lt;p&gt;You&amp;rsquo;ll want to check what kind of ElastAlert rule you want to create, which you can find &lt;a href=&#34;https://elastalert2.readthedocs.io/en/latest/ruletypes.html#rule-types&#34;&gt;here ( https://elastalert2.readthedocs.io/en/latest/ruletypes.html#rule-types )&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;For many of our SIEM Detection rules we use the ElastAlert &lt;code&gt;any&lt;/code&gt; rule type.  According to the ElastAlert documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The any rule will match everything. Every hit that the query returns will generate an alert.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In many cases this will not be what you want as it could generate a lot of noise, but in the case of SIEM Detections, if they&amp;rsquo;re tuned well, hopefully they won&amp;rsquo;t be generating hundreds of records that this ElastAlert rule would be alerting on.&lt;/p&gt;
&lt;p&gt;Below is an example ElastAlert rule that alerts us when there are Azure Subscription level IAM changes (as detected by a SIEM Detection rule).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;name: Azure Subscription IAM Change

index: .siem-signals-default-*

filter:
- query:
    query_string:
      query: &#39;signal.rule.name: &amp;quot;Azure Subscription IAM Change&amp;quot; AND event.outcome: *&#39;

type: any

realert:
  minutes: 0

alert:
- &amp;quot;slack&amp;quot;

alert_subject: &amp;quot;Azure Subscription IAM Change&amp;quot;

alert_text: &amp;quot;
{0}\n
Grantor:\n
User Name: {1}\n
Application Name: {2}\n
Application ID: {3}\n
\n
Grantee:\n
Principal Name: {4}\n
Principal ID: {5}\n
Principal Type: {6}\n
Role Name: {7}\n
Role ID: {8}\n
Subscription Name: {9}\n
Subscription ID: {10}\n
\n&amp;quot;

alert_missing_value: &amp;quot;N/A&amp;quot;

alert_text_args:
- &amp;quot;signal.rule.name&amp;quot;
- &amp;quot;user.name&amp;quot;
- &amp;quot;aad.application.name&amp;quot;
- &amp;quot;aad.application.id&amp;quot;
- &amp;quot;azure.iam.principal.name&amp;quot;
- &amp;quot;azure.iam.principal.id&amp;quot;
- &amp;quot;azure.iam.principal.type&amp;quot;
- &amp;quot;azure.iam.role.name&amp;quot;
- &amp;quot;azure.iam.role.id&amp;quot;
- &amp;quot;azure.subscription.name&amp;quot;
- &amp;quot;azure.subscription.id&amp;quot;

alert_text_type: alert_text_only
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;Note: ElastAlert uses the older Lucene query syntax, whereas modern Kibana uses Kibana Query Language (KQL) by default.  Make sure to switch to using Lucene in Kibana when exploring or writing searches for use with ElastAlert.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;ElastAlert has an extensive set of possible alert targets, in the example I&amp;rsquo;m using Slack, but a few of the other common ones I use are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;HTTP POST&lt;/li&gt;
&lt;li&gt;Command&lt;/li&gt;
&lt;li&gt;Alerta (quite useful alert dashboard)&lt;/li&gt;
&lt;li&gt;Email&lt;/li&gt;
&lt;li&gt;Jira&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The full list can be found here: &lt;a href=&#34;https://elastalert2.readthedocs.io/en/latest/ruletypes.html#alerts&#34;&gt;ElastAlert Alerters&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;notes&#34;&gt;Notes&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;ElastAlert2 is the community fork of the original Yelp created ElastAlert which they abandoned a year or two ago, without any real effort to hand over to anyone to maintain.&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Using Elasticsearch Upserts to Combine Multiple Event Lines Into One</title>
      <link>/posts/using-elasticsearch-upserts-to-combine-multiple-event-lines-into-one/</link>
      <pubDate>Tue, 24 Nov 2020 07:39:19 +0000</pubDate>
      
      <guid>/posts/using-elasticsearch-upserts-to-combine-multiple-event-lines-into-one/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;Note
This approach is probably not appropriate for high volume / high throughput events.  It required in my case quite a lot of Logstash parsing, and Elasticsearch &lt;code&gt;doc_as_upsert&lt;/code&gt; use, both of which will have a significant performance penalty.  For low throughput use it works fine.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Sometimes log sources split logically grouped events into separate lines, and sometimes those logically grouped event lines are mixed into the same log file with actual singular line events.&lt;/p&gt;
&lt;p&gt;This particular case is not dealt with well by Filebeat multiline support&lt;sup&gt;1&lt;/sup&gt;.  In fact it simply doesn&amp;rsquo;t work in this case.&lt;/p&gt;
&lt;p&gt;The structure I&amp;rsquo;m talking about is this:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;Line 1
Line 2
Line 3 - Some event starts
Line 4 - Content of event
Line 5 - End of event
Line 6
Line 7
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Where Lines 1 and 2 are individual events, Lines 3, 4, and 5 are actually multiple lines of the same event, and Lines 6 and 7 are separate individual events again.&lt;/p&gt;
&lt;p&gt;Since Filebeat doesn&amp;rsquo;t deal with this type of setup, at all, I had to look elsewhere to see if I could combine Lines 3, 4, and 5 into one event.&lt;/p&gt;
&lt;p&gt;Logically the next place to look would be Logstash, as we have it in our ingestion pipeline and it has multiline capabilities.  However, we use a set of Azure Event Hubs (essentially Kafka for those not familiar) as our event queueing mechanism, with a group of Logstash processes consuming the events as they arrive.  There&amp;rsquo;s no grouping or ordering here, so Lines 3,4,5 may arrive:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;out of order in time&lt;/li&gt;
&lt;li&gt;across multiuple different Logstash consumers&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This means it impossible to combine these 3 lines into one event, as we may never see all 3 lines at a single logstash process/instance, and therefore can&amp;rsquo;t combine them.&lt;/p&gt;
&lt;p&gt;So they can&amp;rsquo;t be combined at source using Filebeat, and they can&amp;rsquo;t be combined during processing by using Logstashs multiline codec, which only leaves one place where all 3 lines will be in the same place: Elasticsearch itself.&lt;/p&gt;
&lt;p&gt;The approach I settled on was using (or perhaps abusing) Elasticseach&amp;rsquo;s &lt;code&gt;doc_as_upsert&lt;/code&gt;&lt;sup&gt;2&lt;/sup&gt; capability to incrementally add data to a single ES document.&lt;/p&gt;
&lt;p&gt;The key is to identify something that can group the multiple lines together, and use that information as the Document ID.&lt;/p&gt;
&lt;p&gt;In my case, we have the following:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Two common &amp;ldquo;phrases&amp;rdquo; in the event line, such that I can identify all lines reliably as being part of a logical group (i.e. they need to be processed as per the next step)&lt;/li&gt;
&lt;li&gt;A set of datetime and ip/port information thats common across the event lines, that can be used to create a shared &amp;ldquo;signature&amp;rdquo; (using Logstash fingerprint filter)&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;The first step is to identify the common &amp;ldquo;phrases&amp;rdquo; that identify the event lines, and mark each event as part of an &amp;ldquo;upsert&amp;rdquo;.  I do this as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if ( [message] =~ /phrase1/ ) or ( [message] =~ /phrase2/ ) {
    mutate {
      add_tag =&amp;gt; [ &amp;quot;_upserts&amp;quot; ]
	}
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The next step is to parse the common data of each event into a structure that allows the use of Logstash&amp;rsquo;s fingerprint filter.  I extract the datetime and ip/port information, and use Logstash&amp;rsquo;s &lt;code&gt;fingerprint&lt;/code&gt; filter to create an ECS style &lt;code&gt;[event][id]&lt;/code&gt; field.&lt;/p&gt;
&lt;p&gt;Fingerprint the event line:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;    fingerprint {
      method =&amp;gt; &amp;quot;SHA1&amp;quot;
      source =&amp;gt; [
        &amp;quot;[tmp_date_day]&amp;quot;,
        &amp;quot;[tmp_date_month]&amp;quot;,
        &amp;quot;[tmp_date_daynum]&amp;quot;,
        &amp;quot;[tmp_date_time]&amp;quot;,
        &amp;quot;[tmp_date_year]&amp;quot;,
        &amp;quot;[tmp_source_ip]&amp;quot;,
        &amp;quot;[tmp_source_port]&amp;quot;
      ]
      target =&amp;gt; &amp;quot;[event][id]&amp;quot;
    }
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;In my Elasticsearch outputs I then simply filter for that tag, and set a few paramters, as below.  This uses the &lt;code&gt;[event][id]&lt;/code&gt; as the document ID, and will perform an update if a document exists already with the same document id:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;if &amp;quot;_upserts&amp;quot; in [tags] {
  elasticsearch {
    hosts =&amp;gt; [
      &amp;quot;es&amp;quot;
    ]
    index =&amp;gt; &amp;quot;&amp;lt;target index&amp;gt;&amp;quot;
    document_id =&amp;gt; &amp;quot;%{[event][id]}&amp;quot;
    doc_as_upsert =&amp;gt; true
    action =&amp;gt; &amp;quot;update&amp;quot;
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;That&amp;rsquo;s it.  Abusing Elasticsearch Update API to combine multiline log events into one document.  Please don&amp;rsquo;t do this if you have better options available!&lt;/p&gt;
&lt;h3 id=&#34;references&#34;&gt;References:&lt;/h3&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://github.com/elastic/beats/pull/4019&#34;&gt;https://github.com/elastic/beats/pull/4019&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#doc_as_upsert&#34;&gt;https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-update.html#doc_as_upsert&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
</description>
    </item>
    
    <item>
      <title>Event Threat Enrichment using Logstash and Minemeld</title>
      <link>/posts/event-threat-enrichment-logstash-minemeld/</link>
      <pubDate>Fri, 25 Sep 2020 09:22:36 +0100</pubDate>
      
      <guid>/posts/event-threat-enrichment-logstash-minemeld/</guid>
      <description>&lt;p&gt;At my work we use the Elastic Stack for quite a few things, but one of the more recent-ish &amp;ldquo;official&amp;rdquo; roles is as our SIEM.  Elastic introduced SIEM specific funcationality to Kibana a few releases ago, around 7.4 if I rembember correctly.&lt;/p&gt;
&lt;p&gt;One of the features that the Elastic Stack doesn&amp;rsquo;t really support well (yet) is an enrichment system.  They did introduce an elasticsearch side &lt;a href=&#34;https://www.elastic.co/blog/introducing-the-enrich-processor-for-elasticsearch-ingest-nodes&#34;&gt;enrichment system&lt;/a&gt; in 7.5, but in my opinionn theres a few problems with it:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It runs inside ElasticSeach using ES pipelines, which are harder to design and operate than something Logstash side, as wellas not quite being as flexible I&amp;rsquo;d like.&lt;/li&gt;
&lt;li&gt;As it runs in ES ingest nodes, it has a license cost impact&lt;/li&gt;
&lt;li&gt;The processors available have some limitations, such as no support for range queries currently, which is important in the use case I&amp;rsquo;ll be writing about here.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One of the requirements I have for our SIEM is to be able to identify events coming from &amp;ldquo;known risky IPs&amp;rdquo;.  Those risky IPs could be known botnets, spam sources, tor exit nodes, or anything else that you or threat intelligence providers/feeds classify as a risk.&lt;/p&gt;
&lt;p&gt;After much research I settled on the approach I&amp;rsquo;ll detail below.&lt;/p&gt;
&lt;p&gt;First I needed a way of collecting threat feeds in such a way that they&amp;rsquo;d be useable by the SIEM.  For this I settled on using &lt;a href=&#34;https://www.paloaltonetworks.com/products/secure-the-network/subscriptions/minemeld&#34;&gt;Minemeld&lt;/a&gt;, a product by Palo Alto networks, as they describe it &amp;ldquo;an open-source application that streamlines the aggregation, enforcement and sharing of threat intelligence&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;There are quite a few other options for this, but Minemeld seemed ideal for me because:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It&amp;rsquo;s a pretty focused design, where some other options are far far more than just threat feed aggregation.&lt;/li&gt;
&lt;li&gt;It&amp;rsquo;s pretty simple to use and setup&lt;/li&gt;
&lt;li&gt;It has support for delivering threat feed data to Logstash&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;minemeld&#34;&gt;Minemeld&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Note
This post only deals with IoCs of &amp;ldquo;IP type&amp;rdquo;.  Minemeld can consume, aggregate, and distribute IoCs of other tyes, such as URLs, domains, etc, but they are not dealt with by this article.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;How to install, run and configure feeds/aggregators/etc in Minemeld is an exercise left to the reader.&lt;/p&gt;
&lt;p&gt;To enable Logstash output, you can use the built in prototype &lt;code&gt;stdlib.localLogStash&lt;/code&gt;, if your logstash instance is running on the same system as Minemeld and is reachable over localhost:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/234a165e8b86498c8956617b0d01e6ca.png&#34; alt=&#34;234a165e8b86498c8956617b0d01e6ca.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;However in my case, my logstash instances arent runnin on the same host, and in fact I have multiple target instances.&lt;/p&gt;
&lt;p&gt;In a case like this, you need to create a new prototype by pressing the &amp;ldquo;New&amp;rdquo; button highlighted in the above screenshot.&lt;/p&gt;
&lt;p&gt;Set your new &amp;ldquo;local prototype&amp;rdquo; name, and then the important part, set the &lt;code&gt;logstash_host:&lt;/code&gt; config field:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/465f6543348749ac96240d6db28031b0.png&#34; alt=&#34;465f6543348749ac96240d6db28031b0.png&#34;&gt;&lt;/p&gt;
&lt;p&gt;Hit save, and thats it.  You now have a method of outputting IoCs from various IP threat feeds to Logstash.  I&amp;rsquo;ve done this two times, with different &lt;code&gt;logstash_host:&lt;/code&gt; set for each new local prototype.  So I&amp;rsquo;m duplicating my IoCs to two different logstash and elasticsearch clusters.&lt;/p&gt;
&lt;h1 id=&#34;logstash---part-1&#34;&gt;Logstash - Part 1&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Important Note:
You will need to define an index template/mapping that ensures the start and end IP address fields created by the dissect filter below are actually of IP datatype.  IP Range queries used in Part 2 will not work without this.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;On the logstash side the configs for reading and storing the Minemeld provided IoCs are reasonably simple.&lt;/p&gt;
&lt;p&gt;Part 2 will look at how I do event enrichment.&lt;/p&gt;
&lt;p&gt;First, you need to listen on the TCP input port configured in your Minemeld local prototypes, in the case above:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;input {
  tcp {
    port =&amp;gt; 5514
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You then need some filters to parse the message field, create a predictable document ID, and split the &lt;code&gt;@indicator&lt;/code&gt; into a start and end IP address, which will be used for ES range queries in Part 2.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filter {
  # Parse the message field as JSON into the target minemeld field
  json {
    source =&amp;gt; &amp;quot;message&amp;quot;
    target =&amp;gt; &amp;quot;minemeld&amp;quot;
  }
  
  # Generate a fingerprint on the @indicator field, this will be used as the Document ID in the Elasticsearch outputs.
  fingerprint {
    source =&amp;gt; &amp;quot;[minemeld][@indicator]&amp;quot;
    target =&amp;gt; &amp;quot;[@metadata][fingerprint]&amp;quot;
    method =&amp;gt; &amp;quot;MURMUR3&amp;quot;
  }

  # Split the IP indicator field on the -, so we have a start and end IP address
  dissect {
    mapping =&amp;gt; { &amp;quot;[minemeld][@indicator]&amp;quot; =&amp;gt; &amp;quot;%{[minemeld][indicator][ip][start]}-%{[minemeld][indicator][ip][end]}&amp;quot; }
  }

}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So now you just need to store that docuement in an ES index.  The slightly different part, as compared to for example storing log data in ES, is that IoCs can and will &amp;ldquo;expire&amp;rdquo;, and therefore need to be removed from the index when that happens.&lt;/p&gt;
&lt;p&gt;Minemeld caters for this by providing a &amp;ldquo;message&amp;rdquo;, and if the value of that message is &amp;ldquo;withdraw&amp;rdquo;, the IoC can be removed.  As can be seen in the Logstash output configs below, when we receive an event with &lt;code&gt;[minemeld][message] == &amp;quot;withdraw&amp;quot;&lt;/code&gt; we issue a delete against the ES index, as indicated by &lt;code&gt;action =&amp;gt; &amp;quot;delete&amp;quot;&lt;/code&gt;.  This uses the predictable document ID we create in the earlier filter, so we know which document to delete.&lt;/p&gt;
&lt;p&gt;Also note the index template specified in the configs, and refer to the note at the start of this section.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;output {
    if [minemeld][message] == &amp;quot;withdraw&amp;quot; {
      elasticsearch {
        hosts =&amp;gt; [
          &amp;quot;http://&amp;lt;es&amp;gt;&amp;quot;
        ]
        index =&amp;gt; &amp;quot;minemeld&amp;quot;
        manage_template =&amp;gt; true
        template_name =&amp;gt; &amp;quot;minemeld&amp;quot;
        template =&amp;gt; &amp;quot;/etc/logstash/minemeld.indextemplate&amp;quot;
        document_id =&amp;gt; &amp;quot;%{[@metadata][fingerprint]}&amp;quot;
        action =&amp;gt; &amp;quot;delete&amp;quot;
      }
    } else {
      elasticsearch {
        hosts =&amp;gt; [
          &amp;quot;http://&amp;lt;es&amp;gt;&amp;quot;
        ]
        index =&amp;gt; &amp;quot;minemeld&amp;quot;
        manage_template =&amp;gt; true
        template_name =&amp;gt; &amp;quot;minemeld&amp;quot;
        template =&amp;gt; &amp;quot;/etc/logstash/minemeld.indextemplate&amp;quot;
        document_id =&amp;gt; &amp;quot;%{[@metadata][fingerprint]}&amp;quot;
      }
    }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;So at this point you should have some Minemeld provided IoCs stored in an Elasticsearch index, which can be used to enrich other events in real time.&lt;/p&gt;
&lt;h1 id=&#34;logstash---part-2&#34;&gt;Logstash - Part 2&lt;/h1&gt;
&lt;p&gt;So now onto how to enrich events.  In my case, certain events come with a &lt;code&gt;source.ip&lt;/code&gt; address, and I then do a ES lookup using the Logstash elasticsearch &lt;em&gt;filter&lt;/em&gt;.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;filter {
      elasticsearch {
          hosts =&amp;gt; [
            &amp;quot;http://&amp;lt;es&amp;gt;/&amp;quot;
          ]
        index =&amp;gt; &amp;quot;minemeld&amp;quot;
        enable_sort =&amp;gt; &amp;quot;false&amp;quot;
        tag_on_failure =&amp;gt; [ &amp;quot;_threat_lookup_failure&amp;quot; ]
        add_tag =&amp;gt; [&amp;quot;_threat_found&amp;quot;]
        query_template =&amp;gt; &amp;quot;/etc/logstash/conf.d/threat_query.json&amp;quot;
        fields =&amp;gt; {
          &amp;quot;[minemeld][sources]&amp;quot; =&amp;gt; &amp;quot;[custom][threat][sources]&amp;quot;
        }
      }
}
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;The query, specified in &lt;code&gt;query_template =&amp;gt; &amp;quot;/etc/logstash/conf.d/threat_query.json&amp;quot;&lt;/code&gt; is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;{
  &amp;quot;query&amp;quot;: {
    &amp;quot;bool&amp;quot;: {
      &amp;quot;filter&amp;quot;: [
        { &amp;quot;range&amp;quot;: { &amp;quot;minemeld.indicator.ip.start&amp;quot;: { &amp;quot;lte&amp;quot;: &amp;quot;%{[source][ip]}&amp;quot; }}},
        { &amp;quot;range&amp;quot;: { &amp;quot;minemeld.indicator.ip.end&amp;quot;: { &amp;quot;gte&amp;quot;: &amp;quot;%{[source][ip]}&amp;quot; }}}
      ]
    }
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;blockquote&gt;
&lt;p&gt;Note
This is where you need to ensure you&amp;rsquo;ve created these fields as IP datatypes, as mentioned previously.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;All this does is excute the above query against the Minemeld ES index, and looks for a match where the Minemeld &lt;code&gt;minemeld.indicator.ip.start&lt;/code&gt; is less than or equal to, and the &lt;code&gt;minemeld.indicator.ip.end&lt;/code&gt; is greater than or equal to the source IP in the event that we&amp;rsquo;re attempting to enrich.&lt;/p&gt;
&lt;p&gt;Basically: &amp;ldquo;if the source IP is between the start and end range provided by Minemeld&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;The actual enrichment is performed by the &lt;code&gt;fields&lt;/code&gt; parameter of the &lt;code&gt;elasticsearch&lt;/code&gt; filter.  In the example above, it sets a field &lt;code&gt;custom.threat.sources&lt;/code&gt; to the value of &lt;code&gt;minemeld.sources&lt;/code&gt; from the document in the Minemeld index, which is a list of source &amp;ldquo;names&amp;rdquo; provided by minemeld.&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;That&amp;rsquo;s it.  We now have an indication if a source IP in an event is considered a threat by various threat feeds, which is obviously very useful for informing any decisions around responses.&lt;/p&gt;
&lt;p&gt;Some closing thoughts.&lt;/p&gt;
&lt;p&gt;Performance.  This approach is almost definitely not scalable to extremely high throughput levels, due to the overhead of network connections between logstash and elasticsearch, and the impact of querying elasticsearch for every event.  It would be signficantly better tyo take advantage of Logstashs&#39; translate filter or its memcache filter.&lt;/p&gt;
&lt;p&gt;However, both the dictionary and memcache filter suffer from the same limitation: theres no (easy?) way of doing range type queries.&lt;/p&gt;
&lt;p&gt;As I mentioned earlier, Elasticsearchs new enrichment pipeline features also doesnt support range queries, at this time.&lt;/p&gt;
&lt;p&gt;Since the Minemeld threat feeds provide IP ranges, the only options are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Use a queryable source that supports range queries on IP addresses (or their integer representation thereof)&lt;/li&gt;
&lt;li&gt;Expand the Minemeld provided IP ranges into lists of singlular IPs.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;I actually experimented with option two, expanding the Minemeld ranges into complete lists of single /32 IPs. Depending on the number of the threat feeds being consumed and their size, you will end up with millions and millions of IP addresses in your new list, which may not be usable in logstash dictionaries or memcache.  YMMV.&lt;/p&gt;
&lt;p&gt;At the time I decided to continue using the Elasticsearch IP range query approach for now, as the number of events I&amp;rsquo;m enriching is low enough to not impact our performance or availability.  However in the future I&amp;rsquo;m going to want to apply this kind of enrichment against a much larger amount of events, so I&amp;rsquo;m going to have to revisit the approach I think.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Querying Cylance Protect Api From Shell</title>
      <link>/posts/querying-cylance-protect-api-from-shell/</link>
      <pubDate>Fri, 11 Sep 2020 17:32:50 +0100</pubDate>
      
      <guid>/posts/querying-cylance-protect-api-from-shell/</guid>
      <description>&lt;p&gt;We use Cylance as our AV type protection.  They&amp;rsquo;re one of the better solutions I&amp;rsquo;ve seen, but theres some strange gaps in my opinion.  There doesn&amp;rsquo;t seem to be a built in method for alerting.  One of the things we&amp;rsquo;d like to be able to alert on is when a devices goes &amp;ldquo;offline&amp;rdquo;, and apparently this information is not provided through Cylance&amp;rsquo;s syslog output.  It is however available from their API.&lt;/p&gt;
&lt;p&gt;Since we use Elasticsearch at Bede, and use it for the basis of a lot of alerting, I wanted to get the device status records from Cylance, into Elasticsearch so I could alert our security team when Cylance agents stopped reporting in.&lt;/p&gt;
&lt;p&gt;After a bit of reading, the Cylance API seemed simple enough so I whipped up a bit of shell using curl, jq, openssl, etc to authenticate/authorize, and be able to hit any of the API endpoints.&lt;/p&gt;
&lt;p&gt;Hopefully someone else find it useful as well, the code can be found here: &lt;a href=&#34;https://github.com/robrankin/bash-cylance-protect-api&#34;&gt;https://github.com/robrankin/bash-cylance-protect-api&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Credit for some very helpful notes to: &lt;a href=&#34;https://gist.github.com/indrayam/dd47bf6eef849a57c07016c0036f5207&#34;&gt;https://gist.github.com/indrayam/dd47bf6eef849a57c07016c0036f5207&lt;/a&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Kibaba Authentication using OAuth2 Proxy in Kubernetes</title>
      <link>/posts/kibaba-oauth-kubernetes/</link>
      <pubDate>Thu, 06 Aug 2020 12:34:50 +0100</pubDate>
      
      <guid>/posts/kibaba-oauth-kubernetes/</guid>
      <description>&lt;blockquote&gt;
&lt;p&gt;NOTE: There appears to be a bug with Kibanas impersonation features, and SIEM detection rules (and possibly elswhere): &lt;a href=&#34;https://github.com/elastic/kibana/issues/74828&#34;&gt;https://github.com/elastic/kibana/issues/74828&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Recently I had reason to want to integrate Kibana with Azure Active Directory for authentication.  This might be easily possible if you have a commercial license with Elastic, but this wasn&amp;rsquo;t the case this time.&lt;/p&gt;
&lt;p&gt;After a little bit of research I found this article, from February 2017:&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://www.elastic.co/blog/user-impersonation-with-x-pack-integrating-third-party-auth-with-kibana&#34;&gt;User Impersonation with X-Pack: Integrating Third Party Auth with Kibana&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Obviously it&amp;rsquo;s starting to get a little long in the tooth, but as long as user impersonation is still supported, the basic outline should work.&lt;/p&gt;
&lt;p&gt;The main trouble with the article is the specific setup used, illustrated by the image:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/oauth_kibana_1.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Problems:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;&lt;code&gt;oauth2_proxy&lt;/code&gt; terminating the browser connection (and possibly TLS)&lt;/li&gt;
&lt;li&gt;&lt;code&gt;oauth2_proxy&lt;/code&gt; running in reverse proxy mode&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This is more what I was looking for:&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/images/Kibana_Oauth2_Kubernetes.jpg&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;For this deployment, Kibana and OAuth2 Proxy would be deployed on Kubernetes, and would be made available behind the standard k8s ingress controller, &lt;a href=&#34;https://github.com/kubernetes/ingress-nginx&#34;&gt;Ingress Nginx&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Therefore the browser connection would be terminated on the Ingress Nginx controller.  As a side note, this takes advantage of &lt;code&gt;cert-manager&lt;/code&gt; to automatically provision and manage TLS certificates, very handy.&lt;/p&gt;
&lt;p&gt;I also didn&amp;rsquo;t really want to run &lt;code&gt;oauth2_proxy&lt;/code&gt; in full reverse proxy mode, as that&amp;rsquo;s the job of the ingress controller.  Don&amp;rsquo;t need more proxies involved here.&lt;/p&gt;
&lt;p&gt;Luckily, nginx has just the feature for this, the &lt;a href=&#34;http://nginx.org/en/docs/http/ngx_http_auth_request_module.html&#34;&gt;ngx_http_auth_request&lt;/a&gt; module.  As per the documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The ngx_http_auth_request_module module (1.5.4+) implements client authorization based on the result of a subrequest. If the subrequest returns a 2xx response code, the access is allowed. If it returns 401 or 403, the access is denied with the corresponding error code. Any other response code returned by the subrequest is considered an error.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So, in theory as long as a header called &lt;code&gt;es-security-runas-user&lt;/code&gt; can be passed to Kibana, with a value of a valid username, and be authrorized to do so by using HTTP Basic auth as a user with rights to impersonate another use, we&amp;rsquo;re good to go.&lt;/p&gt;
&lt;p&gt;The short form is:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;A &amp;ldquo;Service Account&amp;rdquo; is required in Kibana, with a Role that allows it to impersonate users.&lt;/li&gt;
&lt;li&gt;User accounts in Kibana for your users, with whatever roles they require.&lt;/li&gt;
&lt;li&gt;All requests from the ingress controller to Kibana must have 2 headers:
&lt;ul&gt;
&lt;li&gt;Basic Auth header for the &amp;ldquo;Service Account&amp;rdquo;&lt;/li&gt;
&lt;li&gt;An &lt;code&gt;es-security-runas-user&lt;/code&gt; with a valid username created in Kibana.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;So let&amp;rsquo;s get down to it, then, here&amp;rsquo;s the specific configurations required to make this work!&lt;/p&gt;
&lt;p&gt;These will be a mix between Helm chart values, and some &amp;ldquo;raw-ish&amp;rdquo; Kube configs (deployed using the &lt;code&gt;raw&lt;/code&gt; Helm chart).&lt;/p&gt;
&lt;p&gt;OAuth2 Proxy Helm values.yaml&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;image:
  repository: &amp;quot;quay.io/pusher/oauth2_proxy&amp;quot;
  tag: &amp;quot;v6.0.0&amp;quot;
  pullPolicy: &amp;quot;IfNotPresent&amp;quot;

extraArgs:
  provider: &#39;azure&#39;
  email-domain: &#39;&amp;lt;your email domain here&amp;gt;&#39;
  azure-tenant: &#39;&amp;lt;your tenant id here&amp;gt;&#39;
  client-id: &#39;&amp;lt;your client id here&amp;gt;&#39;
  client-secret: &#39;&amp;lt;your client secret here&amp;gt;&#39;

  redirect-url: https://kibana.example.com/oauth2/callback

  cookie-secret: &#39;&amp;lt;cookie secret&amp;gt;&#39;
  cookie-domain: &amp;lt;cookie domain&amp;gt;
  cookie-samesite: none

  set-xauthrequest: true

  session-store-type: redis
  redis-connection-url: &#39;redis://redis-master:6379/0&#39;

  request-logging: true
  auth-logging: true
  standard-logging: true
  silence-ping-logging: true
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;Notes:&lt;/p&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;Ensure your callback URL does NOT have a trailing slash &lt;code&gt;/&lt;/code&gt;.  This caused me some problems.&lt;/li&gt;
&lt;li&gt;To create an app in Azure for OAuth2 Proxy to use, please follow their documentation here: &lt;a href=&#34;https://oauth2-proxy.github.io/oauth2-proxy/auth-configuration#azure-auth-provider&#34;&gt;Azure Auth provider&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;You&amp;rsquo;ll notice the Redis connection config.  Please read &lt;a href=&#34;https://oauth2-proxy.github.io/oauth2-proxy/configuration#configuring-for-use-with-the-nginx-auth_request-directive&#34;&gt;this&lt;/a&gt; section of the docs carefully, particularly if you&amp;rsquo;re using Azure authentication.&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;p&gt;The key is really the &lt;code&gt;set-xauthrequest&lt;/code&gt; config.  As per the documentation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;set X-Auth-Request-User, X-Auth-Request-Email and X-Auth-Request-Preferred-Username response headers (useful in Nginx auth_request mode)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;So when this is enabled, OAuth2 Proxy will return those 3 headers to the nginx subrequest, making their values available to nginx, rather than just returning an HTTP 2xx without them.&lt;/p&gt;
&lt;p&gt;Is this specific case, we&amp;rsquo;re after the &lt;code&gt;X-Auth-Request-Email&lt;/code&gt; which is returned from Azure AD with the users email address, assuming successful authentication.&lt;/p&gt;
&lt;p&gt;We then need to get that header value (&lt;a href=&#34;mailto:some.users@example.com&#34;&gt;some.users@example.com&lt;/a&gt;) into the &lt;code&gt;es-security-runas-user&lt;/code&gt; and pass that to the upstream Kibana instance.&lt;/p&gt;
&lt;p&gt;Nginx Ingress Resources&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: kibana-ingress
    annotations:
      kubernetes.io/ingress.class: nginx
      nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-Email
      nginx.ingress.kubernetes.io/auth-url: &amp;quot;https://$host/oauth2/auth&amp;quot;
      nginx.ingress.kubernetes.io/auth-signin: &amp;quot;https://$host/oauth2/start?rd=$escaped_request_uri&amp;quot;

      nginx.ingress.kubernetes.io/configuration-snippet: |
        proxy_set_header &#39;es-security-runas-user&#39; $authHeader0;
        proxy_set_header Authorization &amp;quot;Basic &amp;lt;your basic auth string&amp;gt;&amp;quot;;
  spec:
    rules:
    - host: kibana.example.com
      http:
        paths:
        - path: /
          backend:
            serviceName: kibana
            servicePort: 5601

- apiVersion: extensions/v1beta1
  kind: Ingress
  metadata:
    name: kibana-oauth2-ingress
    annotations:
      kubernetes.io/ingress.class: nginx
  spec:
    rules:
    - host: kibana.example.com
      http:
        paths:
        - path: /oauth2
          backend:
            serviceName: oauth2-proxy
            servicePort: 80
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;You&amp;rsquo;ll notice there&amp;rsquo;s actually 2 ingresses defined here, for the same hostname, but different paths.  I&amp;rsquo;m not sure this is required, but it appears to be based on my testing.  If you attempt to use a single host, with separate paths for Kibana and OAuth2 Proxy it simply doesnt work.&lt;/p&gt;
&lt;p&gt;In the Nginx Ingress configs, the important pieces are as follows.&lt;/p&gt;
&lt;h1 id=&#34;part-one&#34;&gt;Part One&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;nginx.ingress.kubernetes.io/auth-url: &amp;ldquo;https://$host/oauth2/auth&amp;rdquo;
nginx.ingress.kubernetes.io/auth-signin: &amp;ldquo;https://$host/oauth2/start?rd=$escaped_request_uri&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;code&gt;auth-url&lt;/code&gt; and &lt;code&gt;auth-signin&lt;/code&gt; annotations activate the &lt;code&gt;ngx_http_auth_request_module&lt;/code&gt;, so that every request through this location ( &lt;code&gt;/&lt;/code&gt; ) must be authenticated by the external source specified.  This source is &lt;code&gt;$host/oauth2&lt;/code&gt;, which is the same hostname as kibana, but on the &lt;code&gt;oauth2&lt;/code&gt; path, so its the second of the ingress resources specified above.&lt;/p&gt;
&lt;h1 id=&#34;part-two&#34;&gt;Part Two&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;nginx.ingress.kubernetes.io/auth-response-headers: X-Auth-Request-Email&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The &lt;code&gt;auth-response-headers&lt;/code&gt; annotation simply passes the &lt;code&gt;X-Auth-Request-Email&lt;/code&gt; header that we received from Oauth2 Proxy onto the upstream, Kibana, assuming successful authentication.&lt;/p&gt;
&lt;p&gt;Relevant documentation for the annotation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;nginx.ingress.kubernetes.io/auth-response-headers: &amp;lt;Response_Header_1, &amp;hellip;, Response_Header_n&amp;gt; to specify headers to pass to backend once authentication request completes.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;This by itself doesn&amp;rsquo;t help much, as Kibana has no idea to do anything with that specific header, but the trick is that the ingress controller does this by setting an nginx var to the value of that header as returned by Oauth2 Proxy, and then setting the same header to be passed upstream using &lt;code&gt;proxy_set_header&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Relevant nginx config snippet:&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;auth_request_set $authHeader0 $upstream_http_x_auth_request_email;
proxy_set_header &#39;X-Auth-Request-Email&#39; $authHeader0;
&lt;/code&gt;&lt;/pre&gt;&lt;h1 id=&#34;part-three&#34;&gt;Part Three&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;proxy_set_header &amp;lsquo;es-security-runas-user&amp;rsquo; $authHeader0;
proxy_set_header Authorization &amp;ldquo;Basic &lt;your basic auth string&gt;&amp;quot;;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The important piece for Kibana authentication comes next, by re-using the nginx var &lt;code&gt;$authHeader0&lt;/code&gt;, creating and setting the &lt;code&gt;es-security-runas-user&lt;/code&gt; header using that var, and passing that upstream using &lt;code&gt;proxy_set_header&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;As long as Kibana receives the &lt;code&gt;es-security-runas-user&lt;/code&gt; and the basic auth header &lt;code&gt;Authorization: Basic &amp;lt;your basic auth string&amp;gt;&lt;/code&gt;, it will attempt to &amp;ldquo;login&amp;rdquo; using the value of the &lt;code&gt;es-security-runas-user&lt;/code&gt; header.&lt;/p&gt;
&lt;p&gt;In this case we&amp;rsquo;re using emails as the username, but if your auth source provides a different value that OAuth2 Proxy can return to nginx, the same approach could be used.&lt;/p&gt;
&lt;p&gt;Since this isintegrated with the Azure AD authentication flow, it can also take advantage of other authentication requirements, such as requiring MFA, or even Azure AD&amp;rsquo;s Conditional Access system.  These sorts of features are available on most other auth providers I&amp;rsquo;ve seen as well of course.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
